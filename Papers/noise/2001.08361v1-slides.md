# Scaling Laws for Neural Language Models
- Jared Kaplan, John Hopkins University, OpenAI
- Sam McCandlish, OpenAI
- Tom Henighan, OpenAI
- Tom B. Brown, OpenAI
- Benjamin Chess, OpenAI
- Rewon Child, OpenAI
- Scott Gray, OpenAI
- Alec Radford, OpenAI
- Jeffrey Wu, OpenAI
- Dario Amodei, OpenAI
- January 23, 2020

# Introduction
- Study of empirical scaling laws for language model performance
- Focus on cross-entropy loss
- Analysis spans model size, dataset size, and compute used for training
- Trends observed over seven orders of magnitude
- Minimal effects from architectural details within a wide range

# Background and Related Work
- Significant progress in deep learning for language modeling
- State-of-the-art models approaching human-level performance
- Focus on the Transformer architecture
- Examination of performance trends over large scales

# Contributions
- Empirical investigation of scaling laws for language models
- Identification of power-law relationships with model size, dataset size, and compute
- Analysis of overfitting and training speed relationships
- Recommendations for optimal allocation of compute budget

# Objective
- Determine how language modeling performance scales with key factors
- Focus on Transformer architecture
- Explore power-law relationships governing performance

# Methodology Overview
- High-level approach: empirical study of scaling laws
- Data: WebText2 dataset
- Model architecture: Transformer

# Datasets
- WebText2 dataset: extended version of WebText
- Tokenized using byte-pair encoding
- Dataset size: 96 GB of text, 1.62 x 10^10 words, 2.29 x 10^10 tokens
- Reserved tokens for test set: 6.6 x 10^8

# Model Details
- Transformer architecture parameterized by layers, dimensions, and attention heads
- Model size defined as number of non-embedding parameters
- Forward pass compute estimation: 2N + 2n_layer * n_ctx * d_model

# Experiments
- Variety of models trained, varying model size, dataset size, and other factors
- Training procedure: Adam optimizer, 2.5 x 10^5 steps, batch size of 512 sequences
- Learning rate schedule: linear warmup, cosine decay

# Results
- Language modeling performance improves with increased model size, dataset size, and compute
- Performance follows power-law relationship when not bottlenecked by other factors
- Larger models are more sample-efficient
- Compute-efficient training involves large models and early stopping

# Performance Comparisons with Prior Work
- Comparison to LSTMs and Universal Transformers
- Transformers outperform LSTMs, especially with longer contexts
- Consistent improvement in performance with increased model size

# Ablation Studies
- Weak dependence on architectural hyperparameters (depth, width, attention heads)
- Performance trends hold across different model shapes and sizes

# Visualizations
- Performance trends with model size and dataset size
- Comparison of empirical trends to theoretical predictions
- Critical batch size and training efficiency visualizations

# Discussion
- Key insights: power-law scaling, compute efficiency, sample efficiency
- Interpretation of results: larger models and efficient training methods
- Limitations: potential overfitting, need for more data with larger models

# Conclusion
- Summary of findings: scaling laws, power-law relationships, efficient training
- Impact: larger models perform better and are more sample-efficient
- Future work: exploration of other domains, theoretical framework for scaling laws

# References
- Relevant citations from the paper:
  - Vaswani et al., 2017 (Transformer architecture)
  - Devlin et al., 2018 (BERT)
  - Radford et al., 2019 (GPT-2)
  - Yang et al., 2019 (XLNet)
  - Liu et al., 2019 (RoBERTa)

# Acknowledgements
- Thanks to Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner, Danny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and feedback

# Q&A
- Invitation for questions
