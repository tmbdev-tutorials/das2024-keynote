# Language Models are Few-Shot Learners

- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei
- OpenAI
- Date of Presentation: July 22, 2020

# Introduction

- Substantial gains in NLP tasks via pre-training followed by fine-tuning
- Task-specific fine-tuning datasets required in large quantities
- Humans can perform new language tasks from few examples
- GPT-3: 175 billion parameter model tested in few-shot settings
- Strong performance without gradient updates or fine-tuning

# Background/Related Work

- Single-layer word vectors to RNNs for stronger representations
- Pre-trained recurrent/transformer models fine-tuned for tasks
- Limitations of task-specific fine-tuning datasets
- Meta-learning and in-context learning as potential solutions

# Contributions

- Training and evaluating GPT-3, an autoregressive language model
- Few-shot learning performance comparable to state-of-the-art fine-tuning
- Identifying tasks and datasets where GPT-3 struggles
- Generating human-like news articles with GPT-3
- Discussing societal impacts of GPT-3

# Objective

- Main objective: Evaluate GPT-3's in-context learning abilities
- Hypotheses: Scaling up language models improves few-shot performance

# Methodology Overview

- Training a 175 billion parameter autoregressive language model
- Evaluating on over two dozen NLP datasets
- Few-shot, one-shot, and zero-shot settings
- No gradient updates or fine-tuning during evaluation

# Datasets

- Common Crawl, WebText2, Books1, Books2, Wikipedia
- Filtering Common Crawl for quality
- Deduplication to prevent redundancy
- High-quality datasets added to training mix

# Model Details

- GPT-3 architecture: 96 layers, 12288 units per layer, 96 attention heads
- Context window of 2048 tokens
- Training on 300 billion tokens
- Alternating dense and sparse attention patterns

# Experiments

- Evaluating on language modeling, cloze tasks, completion tasks
- Testing on translation, question-answering, and reasoning tasks
- Few-shot, one-shot, and zero-shot settings
- Measuring and preventing memorization of benchmarks

# Results

- Strong performance in few-shot settings across tasks
- Zero-shot and one-shot settings also show promising results
- Some tasks where GPT-3 struggles, e.g., natural language inference
- Generating human-like news articles

# Performance Comparisons with Prior Work

- GPT-3 achieves state-of-the-art on some tasks without fine-tuning
- Comparison with fine-tuned models on benchmarks
- Performance gains observed with model scaling

# Ablation Studies

- Effect of different model components
- Performance differences with varying context sizes
- Analyzing model's few-shot learning capabilities

# Visualizations

- Accuracy curves for various tasks
- Performance scaling with model size
- In-context learning curves

# Discussion

- Key insights into GPT-3's capabilities and limitations
- Interpretation of few-shot learning results
- Addressing data contamination concerns
- Broader societal impacts

# Conclusion

- Summary of GPT-3's contributions and performance
- Potential future directions: model scaling, fine-tuning, and grounding
- Importance of ethical considerations and bias mitigation

# References

- Vaswani et al., "Attention is All You Need"
- Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
- Radford et al., "Language Models are Unsupervised Multitask Learners"
- Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"

# Acknowledgements

- Thanks to contributors, OpenAI infrastructure team, and content creators

# Q&A

- Invitation for questions and further discussion
