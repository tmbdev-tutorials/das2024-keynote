# Title Slide
- Training Compute-Optimal Large Language Models
- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre
- DeepMind
- Date of Presentation

# Introduction
- Investigate optimal model size and number of tokens for training transformer language models under a compute budget
- Current large language models are significantly under-trained
- Key finding: model size and number of training tokens should be scaled equally
- Introduce Chinchilla, a compute-optimal model, and compare it with Gopher, GPT-3, Jurassic-1, and Megatron-Turing NLG

# Background/Related Work
- Large Language Models (LLMs) have demonstrated impressive performance (Brown et al., 2020; Lieber et al., 2021; Rae et al., 2021; Smith et al., 2022; Thoppilan et al., 2022)
- Substantial compute and energy costs are associated with training large models
- Kaplan et al. (2020) showed a power law relationship between model size and performance, suggesting larger models for performance improvements

# Contributions
- Empirical investigation of optimal model size and number of tokens for a given compute budget
- Training of over 400 language models to estimate compute-optimal model configurations
- Introduction of Chinchilla, a compute-optimal model that outperforms larger models

# Objective
- Determine the optimal trade-off between model size and the number of training tokens given a fixed compute budget
- Empirical estimation of the functions \(N_{\text{opt}}(C)\) and \(D_{\text{opt}}(C)\) for optimal compute allocation

# Methodology Overview
- Training of 400+ language models with varying parameters and tokens
- Empirical modeling of pre-training loss as a function of model parameters and training tokens
- Analysis of training data to fit an empirical estimator for optimal scaling

# Datasets
- MassiveText dataset used for training both Chinchilla and Gopher
- Slightly different subset distribution to account for increased number of training tokens

# Model Details
- Chinchilla: 70 billion parameters, trained on 1.4 trillion tokens
- Gopher: 280 billion parameters, trained on 300 billion tokens
- Use of AdamW optimizer and a modified SentencePiece tokenizer for Chinchilla

# Experiments
- Comparison of Chinchilla with Gopher, GPT-3, Jurassic-1, and Megatron-Turing NLG
- Evaluation on language modeling, reading comprehension, question answering, common sense, MMLU, and BIG-bench tasks

# Results
- Chinchilla significantly outperforms Gopher and other large models on various evaluation tasks
- Achieves state-of-the-art accuracy on the MMLU benchmark with 67.5%

# Performance Comparisons with Prior Work
- Chinchilla outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B)
- Substantially less compute required for fine-tuning and inference with Chinchilla

# Discussion
- Importance of scaling both model size and number of training tokens equally
- Need for larger, high-quality datasets for further scaling of language models
- Ethical and privacy concerns with training on large datasets

# Conclusion
- Chinchilla demonstrates that optimal scaling leads to better performance with less compute
- Highlights the need for a balanced approach to scaling model size and training data
- Future work should focus on dataset quality and ethical considerations

# References
- Brown et al., 2020; Lieber et al., 2021; Rae et al., 2021; Smith et al., 2022; Thoppilan et al., 2022
- Kaplan et al., 2020
- Hendrycks et al., 2020
- Gao et al., 2020

# Acknowledgements
- Thanks to Jean-baptiste Alayrac, Kareem Ayoub, Chris Dyer, Nando de Freitas, Demis Hassabis, Geoffrey Irving, Koray Kavukcuoglu, Nate Kushman, and Angeliki Lazaridou
- DeepMind colleagues for helpful discussions
- JAX and XLA team for their support

# Q&A
- Invitation for Questions
