# ONE SENTENCE SUMMARY:
Chinchilla, a 70B parameter model trained on 1.4T tokens, outperforms larger models like Gopher and GPT-3 by optimizing compute usage and model scaling.

# MAIN POINTS:
1. Current large language models are significantly under-trained.
2. Optimal compute usage requires scaling model size and training tokens equally.
3. Chinchilla outperforms larger models like Gopher and GPT-3.
4. Chinchilla uses 70B parameters and 4x more data than Gopher.
5. Chinchilla achieves 67.5% accuracy on the MMLU benchmark.
6. Training Chinchilla involved over 400 models tested.
7. Chinchilla uses less compute for fine-tuning and inference.
8. Current models are oversized given their compute budgets.
9. Chinchilla's training setup differed in optimizer and tokenizer details.
10. Optimal model scaling needs high-quality, large datasets.

# TAKEAWAYS:
1. Equally scale model size and training tokens for optimal compute usage.
2. Smaller, optimally trained models can outperform larger ones.
3. Chinchilla achieves state-of-the-art performance on multiple benchmarks.
4. High-quality, large datasets are crucial for model performance.
5. Efficient compute usage reduces downstream costs and hardware requirements.
