# Resilience of Large Language Models for Noisy Instructions

- Bin Wang, Chengwei Wei, Zhengyuan Liu, Geyu Lin, Nancy F. Chen
- Institute for Infocomm Research (I2R), A*STAR, Singapore
- CNRS@CREATE, Singapore
- University of Southern California, USA
- Centre for Frontier AI Research (CFAR), A*STAR, Singapore
- Date of Presentation: April 15, 2024

# Introduction

- Large language models (LLMs) excel in understanding and generating human-like text
- Real-world applications often include noisy instructions
- Investigates resilience of LLMs against five types of noise
- Emphasizes importance of enhancing model resilience

# Background/Related Work

- Previous studies on prompt sensitivity and noisy text reconstruction
- Existing LLMs like ChatGPT, Gemini, LLaMa, Mistral
- Sensitivity to semantically similar inputs and prompt engineering
- Related studies on error correction for ASR, OCR, and grammatical mistakes

# Contributions

- Investigates five common types of disruptions: ASR, OCR, grammatical mistakes, typographical errors, distractive content
- Evaluates a "re-pass" strategy for purifying instructions
- Highlights challenges in correcting noisy instructions for open-source LLMs

# Objective

- Assess the resilience of LLMs to various types of noise
- Explore the effectiveness of error correction strategies
- Understand the impact of noise on model performance

# Methodology Overview

- Analyzed noise within user instructions using GPT-4
- Introduced five types of noise into instructions
- Evaluated model performance with noisy instructions
- Implemented a "re-pass" strategy for noise correction

# Datasets

- ShareGPT dataset for user inputs
- CommonVoice-15 for ASR errors
- NLPAug for OCR and typographical errors
- JELEG and C4-200M for grammatical mistakes
- ShareGPT for distractive content

# Experiments

- Employed hybrid rule-based techniques and generative models to introduce noise
- Categorized noisy instructions based on Word Error Rate (WER)
- Evaluated models: ChatGPT-3.5, Mistral-7B-Instruct-v0.2, Llama-2-7B-Chat

# Results

- Performance significantly declines with noisy instructions
- ASR errors cause notable accuracy drops
- OCR errors disrupt tokenization and semantic representation
- Grammatical mistakes have lesser impact due to model robustness
- Typographical errors severely influence performance

# Performance Comparisons with Prior Work

- ChatGPT-3.5 demonstrates better error correction capabilities
- Open-source models like Llama-2-7B-Chat show performance drops post-correction
- Resilience to ASR and OCR errors needs improvement

# Visualizations

- Figure showing the impact of different noise types on model performance
- Performance evaluation with varying WER for ASR and OCR errors
- Cooperative and non-cooperative distractive content impact

# Discussion

- Models lack robustness against ASR and OCR errors
- Grammatical errors are easier to correct
- Non-cooperative distractive content has more disruptive impact
- Need for task-agnostic, lightweight models for noisy instruction correction

# Conclusion

- LLMs need stronger resilience and noise correction capabilities
- Future work should focus on system integration and multilingual scenarios
- Efficient correction methods are crucial for real-world applications

# References

- Achiam, J. et al. (2023). GPT-4 Technical Report.
- Ardila, R. et al. (2020). Common Voice: A Massively-Multilingual Speech Corpus.
- BÃ¼hrke, J. et al. (2021). Is Making Mistakes Human? On the Perception of Typing Errors in Chatbot Communication.
- Chiang, W. et al. (2023). Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality.
- Touvron, H. et al. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models.

# Acknowledgements

- A*STAR, Singapore
- CNRS@CREATE, Singapore
- University of Southern California, USA
- Centre for Frontier AI Research (CFAR), A*STAR, Singapore

# Q&A

- Invitation for Questions
