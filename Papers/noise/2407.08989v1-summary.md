# ONE SENTENCE SUMMARY:
Large language models (LLMs) show strong resilience to various text perturbations, challenging the assumption that clean data is essential for NLP tasks.

# MAIN POINTS:
1. Clean datasets are rare in real-world NLP scenarios.
2. LLMs have achieved remarkable performance in many NLP tasks.
3. Robustness of LLMs to text noise is crucial but sparsely studied.
4. The study evaluates LLMs' resilience to corrupted text variations.
5. Generative LLMs are quite robust to common text perturbations.
6. LLMs achieved new state-of-the-art in grammar error correction.
7. Human-annotated dataset for LLM vs. human-corrected outputs is released.
8. Noise in datasets can originate from both human and machine errors.
9. Lexical Semantic Change (LSC) detection is essential for understanding semantic shifts.
10. Prompting techniques and various perturbations are used to test LLM robustness.

# TAKEAWAYS:
1. LLMs handle text perturbations well, maintaining semantic integrity.
2. Generative LLMs can outperform humans in grammar error correction.
3. Real-world noise in data does not significantly degrade LLM performance.
4. LLMs' robustness suggests a future where language error correction may be unnecessary.
5. Comprehensive evaluation of LLMs' performance on noisy data is essential for reliable NLP deployment.
