# LayoutLM: Pre-training of Text and Layout for Document Image Understanding

- Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou
- Harbin Institute of Technology, Beihang University, Microsoft Research Asia
- Date of Presentation: August 23-27, 2020

# Introduction

- Document AI involves techniques for reading, understanding, and analyzing business documents
- Business documents vary in format: digital-born or scanned
- Manually extracting data from business documents is time-consuming and costly
- Aim: Automate document processing using AI models
- Challenges: Diversity of layouts, poor quality of scanned images, complex templates

# Background/Related Work

- Early methods: Rule-based and conventional machine learning approaches
- Recent methods: Deep learning approaches using CNNs and R-CNN models
- Limitations: Reliance on limited labeled data, separate pre-training of text and layout information

# Contributions

- First framework to pre-train text and layout jointly for document image understanding
- Uses Masked Visual-Language Model (MVLM) and Multi-label Document Classification (MDC) as training objectives
- Achieves state-of-the-art results in multiple tasks: form understanding, receipt understanding, document classification

# Objective

- Main objective: Improve document image understanding by jointly modeling text and layout information
- Hypothesis: Incorporating 2-D position and image embeddings will enhance performance on document understanding tasks

# Methodology Overview

- High-level approach: Extend BERT to include 2-D position and image embeddings
- Data: Pre-trained on IIT-CDIP Test Collection with over 6 million documents
- Model architecture: Transformer-based with additional embeddings for layout and image features

# Datasets

- **Pre-training Dataset**: IIT-CDIP Test Collection with 11 million scanned document images
- **Fine-tuning Datasets**:
  - FUNSD for form understanding
  - SROIE for receipt information extraction
  - RVL-CDIP for document image classification

# Model Details

- **Architecture**: Transformer-based, extended from BERT
- **Key Components**:
  - 2-D position embeddings for layout information
  - Image embeddings for visual features
- **Training Procedure**:
  - Pre-training with MVLM and MDC losses
  - Fine-tuning on specific tasks with end-to-end updates

# Experiments

- **Experimental Setup**:
  - Pre-trained models: LayoutLMBASE and LayoutLMLARGE
  - Evaluation metrics: F1 score for form and receipt understanding, accuracy for document classification

# Results

- **Form Understanding (FUNSD)**:
  - LayoutLMBASE: 78.66 F1 (11M documents, 2 epochs)
  - LayoutLMLARGE: 79.27 F1 (11M documents, 2 epochs)

- **Receipt Understanding (SROIE)**:
  - LayoutLMLARGE: 95.24 F1 (11M documents, 2 epochs)

- **Document Classification (RVL-CDIP)**:
  - LayoutLMBASE: 94.42% accuracy (11M documents, 2 epochs)

# Performance Comparisons with Prior Work

- LayoutLM outperforms BERT and RoBERTa in all evaluated tasks
- Significant improvements in accuracy and F1 scores over previous state-of-the-art models

# Conclusion

- LayoutLM effectively combines text and layout information for document image understanding
- Demonstrates significant performance gains across multiple tasks
- Future work: Extend pre-training with more data, improve network architectures, explore new training objectives

# References

- Key citations related to document analysis, pre-training models, and deep learning approaches

# Acknowledgements

- Contributions from co-authors and support from Microsoft Research Asia

# Q&A

- Invitation for questions
