# LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding

- Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou
- Harbin Institute of Technology, Microsoft Research Asia, Microsoft Azure AI, Soochow University
- Date of Presentation

# Introduction

- Visually-rich Document Understanding (VrDU) aims to analyze business documents
- Documents include invoices, forms, etc., where structured information is extracted
- Combines textual, visual, and layout information for accurate recognition
- Cross-modality modeling is essential for effective VrDU

# Background/Related Work

- Previous VrDU approaches either fuse information shallowly or deeply
- Shallow fusion combines pre-trained NLP and CV models
- Deep fusion uses pre-training techniques to learn cross-modality interaction end-to-end
- LayoutLMv2 follows the deep fusion direction to improve pre-training strategies

# Contributions

- Proposes a multi-modal Transformer model integrating text, layout, and visual information
- Introduces spatial-aware self-attention mechanism in the Transformer architecture
- Adds new pre-training strategies: text-image alignment and text-image matching
- Achieves state-of-the-art results on multiple VrDU tasks

# Objective

- Improve pre-training strategies for VrDU tasks
- Enhance cross-modality interaction learning in a single framework
- Utilize spatial-aware self-attention to better model document layout

# Methodology Overview

- Multi-modal Transformer architecture
- Uses text, visual, and layout information
- Pre-training tasks: Masked Visual-Language Modeling, Text-Image Alignment, Text-Image Matching
- Spatial-aware self-attention mechanism

# Datasets

- IIT-CDIP: Pre-training dataset with over 11 million scanned document pages
- FUNSD: Form understanding dataset
- CORD: Receipt understanding dataset
- SROIE: Receipt information extraction dataset
- Kleister-NDA: Long document understanding with complex layout
- RVL-CDIP: Document image classification
- DocVQA: Visual question answering on document images

# Model Details

- Multi-modal Transformer architecture with text, visual, and layout embeddings
- Visual embedding uses ResNeXt-FPN for feature extraction
- Spatial-aware self-attention mechanism adds relative position biases
- Pre-training tasks enhance cross-modality interaction learning

# Experiments

- Evaluated on six benchmark datasets
- Experimental setup includes pre-training and fine-tuning phases
- Evaluation metrics: entity-level F1 score, classification accuracy, ANLS score

# Results

- LayoutLMv2 outperforms previous models on all datasets
- Significant improvements in entity extraction tasks and document classification
- Achieves new state-of-the-art results in VrDU tasks

# Performance Comparisons with Prior Work

- LayoutLMv2 vs. BERT, UniLMv2, LayoutLM
- LayoutLMv2 shows superior performance due to multi-modal pre-training
- Outperforms existing multi-modal approaches like BROS, SPADE, PICK, TRIE

# Ablation Studies

- Evaluated the effect of visual information, pre-training tasks, and spatial-aware self-attention
- Both text-image alignment and matching tasks improve model performance
- Spatial-aware self-attention further enhances accuracy

# Visualizations

- ((Figure showing model architecture and pre-training strategies for LayoutLMv2))
- Accuracy curves for different datasets
- Comparison of performance metrics across models

# Discussion

- LayoutLMv2 effectively leverages text, visual, and layout information
- New pre-training tasks and spatial-aware self-attention contribute to performance gains
- Potential for further improvements in multi-modal pre-training

# Conclusion

- LayoutLMv2 integrates text, layout, and visual information in a single framework
- Achieves state-of-the-art results on various VrDU tasks
- Future work includes exploring network architecture and multi-lingual expansion

# References

- Key citations include works on BERT, UniLMv2, ViLBERT, LayoutLM, and relevant VrDU datasets

# Acknowledgements

- Supported by the National Key R&D Program of China and the National Natural Science Foundation of China

# Q&A

- Invitation for questions
