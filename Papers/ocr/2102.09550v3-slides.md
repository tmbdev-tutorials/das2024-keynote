# Title Slide
- Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer
- Rafał Powalski, Łukasz Borchmann, Dawid Jurkiewicz, Tomasz Dwojak, Michał Pietruszka, Gabriela Pałka
- Applica.ai, Poznan University of Technology, Adam Mickiewicz University, Jagiellonian University
- Date of Presentation

# Introduction
- Addressing Natural Language Comprehension beyond plain-text documents
- Introducing TILT neural network architecture
- Simultaneously learns layout, visual features, and textual semantics
- Achieves state-of-the-art results in document information extraction and question answering
- Simplifies process with an end-to-end model

# Background/Related Work
- Most NLP tasks unified under question, context, and answer framework
- Challenges prevalent in business cases: contracts, forms, applications, invoices
- Importance of spatio-visual relations in real-world problems
- Limitations of sequence labeling models in key information extraction

# Contributions
- Introducing TILT: a model combining text, layout, and image modalities
- Achieves state-of-the-art results in DocVQA, CORD, SROIE
- Simplifies end-to-end process for document understanding tasks
- Overcomes limitations of sequence labeling methods

# Objective
- Unify Document Classification, Key Information Extraction, and Question Answering
- Handle documents with complex spatial layouts
- Improve performance on tasks requiring layout understanding

# Methodology Overview
- Based on encoder-decoder Transformer architecture
- Layout represented as attention bias
- Contextualized visual information included
- Uses pretrained encoder-decoder Transformer

# Datasets
- CORD: Indonesian receipts with four categories
- SROIE: Extract values from scanned receipts
- DocVQA: Visual question answering on industry documents
- RVL-CDIP: Classification of industry documents into 16 categories

# Model Details
- Transformer architecture with attention bias for layout
- Contextualized image embeddings using U-Net
- Spatial bias for horizontal and vertical distances
- Visual features added to word embeddings

# Experiments
- Key Information Extraction, Visual Question Answering, Document Classification
- Datasets: CORD, SROIE, DocVQA, RVL-CDIP
- Training procedure: unsupervised pretraining, supervised training, finetuning

# Results
- State-of-the-art results on DocVQA, CORD
- Comparable performance on SROIE, RVL-CDIP
- Robust performance on table-like and yes/no categories in DocVQA
- Improved document classification accuracy on RVL-CDIP

# Performance Comparisons with Prior Work
- Improved scores on DocVQA compared to LayoutLM and LayoutLMv2
- Better F1 scores on CORD and SROIE datasets
- Comparable accuracy on RVL-CDIP with simplified workflow

# Ablation Studies
- Impact of removing spatial bias and visual embeddings
- Importance of regularization techniques: case augmentation, spatial augmentation
- Significance of supervised pretraining on overall performance

# Discussion
- Importance of spatial and image information in document understanding
- Encoder-decoder architecture's advantages over sequence labeling
- Potential for further improvements with larger model variants

# Conclusion
- Novel encoder-decoder framework for layout-aware models
- Achieves state-of-the-art results on multiple datasets
- Simplifies the workflow for document understanding tasks
- Future work: exploring larger model variants and additional datasets

# References
- Key citations: [44], [55], [11], [19], [38]

# Acknowledgements
- Thanks to Filip Graliński, Tomasz Stanisławek, Łukasz Garncarek
- Supported by Smart Growth Operational Programme under project no. POIR.01.01.01-00-0877/19-00

# Q&A
- Invitation for Questions
