# StrucTexT: Structured Text Understanding with Multi-Modal Transformers
- Yulin Li, Yuxi Qian, Yuechen Yu, Xiameng Qin, Chengquan Zhang, Yan Liu, Kun Yao, Junyu Han, Jingtuo Liu, Errui Ding
- Department of Computer Vision Technology (VIS), Baidu Inc.
- Beijing University of Posts and Telecommunications
- Taikang Insurance Group
- MM '21, October 20â€“24, 2021

# Introduction
- Structured text understanding on VRDs is crucial for Document Intelligence
- Tasks involve extracting key information and semantic links from documents
- Challenges due to complexity in content and layout
- Traditional methods split tasks into entity labeling and entity linking

# Background/Related Work
- Existing methods use OCR engines for text detection and labeling
- Named Entity Recognition (NER) models used for sequential labeling
- Challenges in handling both token-level and segment-level information
- Recent works incorporate layout and visual information but with limitations

# Contributions
- Propose a unified framework named StrucTexT for structured text understanding
- Introduce a segment-token aligned encoder based on transformers
- Novel pre-training strategy with three self-supervised tasks
- Demonstrate superior performance on FUNSD, SROIE, and EPHOIE datasets

# Objective
- Efficient extraction of structured data from VRDs at both token and segment levels
- Improve entity labeling and linking by leveraging multi-modal information
- Develop a flexible framework that handles various document structures

# Methodology Overview
- Unified framework with transformer encoder for multi-modal fusion
- Segment-token aligned encoder for different granularity levels
- Pre-training strategy with Masked Visual Language Modeling, Sentence Length Prediction, and Paired Boxes Direction tasks

# Datasets
- FUNSD: 199 real scanned form images, tasks include word grouping, entity labeling, and linking
- SROIE: 973 receipts with predefined values, segment-level text bounding boxes provided
- EPHOIE: 1,494 Chinese examination papers, token-level entity labeling

# Model Details
- Transformer encoder with segment ID embedding for cross-modal alignment
- Visual and textual features combined using Hadamard product
- Three self-supervised tasks for pre-training: MVLM, SLP, PBD

# Experiments
- Extensive experiments conducted on FUNSD, SROIE, and EPHOIE datasets
- Evaluation metrics include precision, recall, F1-score, mAP, mRank, Hit@1, Hit@2, Hit@5

# Results
- StrucTexT outperforms state-of-the-art methods on entity labeling and linking tasks
- Achieves superior performance on SROIE and competitive results on FUNSD
- Demonstrates significant improvements in token-level entity labeling on EPHOIE

# Performance Comparisons with Prior Work
- Outperforms LayoutLMv2_LARGE on SROIE with a smaller model
- Competitive performance against LayoutLMv2_LARGE on FUNSD with fewer pre-training documents
- Superior results in entity linking compared to DocStruct and SPADE

# Ablation Studies
- Impact of individual components and pre-training tasks analyzed
- Multi-modal feature fusion improves performance significantly
- Segment-level representation more effective than token-level for entity labeling

# Discussion
- StrucTexT effectively leverages multi-modal and multi-granularity features
- Pre-training tasks enhance semantic and geometric understanding
- Limitations include handling of ambiguous entities and overfitting to layout positions

# Conclusion
- StrucTexT provides a unified solution for structured text understanding in VRDs
- Novel pre-training strategies enhance multi-modal feature representation
- Future work to focus on goal-directed information aggregation and handling ambiguous entities

# References
- Key citations include works on document structure, information extraction, and pre-training models

# Acknowledgements
- Thanks to the Department of Computer Vision Technology (VIS), Baidu Inc., and Beijing University of Posts and Telecommunications

# Q&A
- Invitation for questions and further discussion
