# TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models

- Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei
- Beihang University, Microsoft Corporation
- Date of Presentation

# Introduction

- Optical Character Recognition (OCR) converts images of text into machine-encoded text.
- Traditional OCR systems use CNN for image understanding and RNN for text generation.
- Language models are often needed to improve accuracy.
- TrOCR leverages pre-trained Transformers for both image and text processing.

# Background/Related Work

- Traditional OCR uses CNNs and RNNs.
- Transformers have shown improvements in text recognition.
- Existing methods still rely on CNN backbones.
- TrOCR aims to replace CNNs with pre-trained image Transformers.

# Contributions

- Introduce TrOCR, an end-to-end Transformer-based OCR model.
- Uses pre-trained image and text Transformers, eliminating the need for CNN backbones.
- Achieves state-of-the-art results on multiple OCR tasks.

# Objective

- Develop an OCR model using pre-trained Transformers.
- Evaluate its performance on printed, handwritten, and scene text recognition tasks.

# Methodology Overview

- TrOCR uses a standard Transformer architecture.
- Encoder uses pre-trained image Transformers.
- Decoder uses pre-trained text Transformers.
- Model pre-trained on synthetic data and fine-tuned on human-labeled datasets.

# Model Details

## Encoder

- Input image resized to 384×384 and split into 16×16 patches.
- Patches flattened into vectors and linearly projected to D-dimensional vectors.
- Position embeddings added to patch embeddings.

## Decoder

- Standard Transformer decoder with encoder-decoder attention.
- Decoder uses self-attention and feed-forward networks.
- Outputs wordpiece tokens using beam search.

# Experiments

- Evaluated on multiple OCR benchmark datasets.
- Compared with state-of-the-art models using CNN and RNN backbones.
- Performed ablation studies on pre-trained models, data augmentation, and pre-training stages.

# Results

- TrOCR outperforms existing state-of-the-art models on SROIE, IAM Handwriting Database, and various scene text datasets.
- Achieves high precision, recall, and F1 scores.
- Demonstrates the effectiveness of Transformer-based OCR without complex pre/post-processing steps.

# Performance Comparisons with Prior Work

- TrOCR models show significant improvements over traditional CNN and RNN-based models.
- Transformer-based approach provides better visual feature extraction and language modeling.

# Conclusion

- TrOCR leverages pre-trained Transformers for OCR, eliminating the need for CNN backbones.
- Achieves state-of-the-art results on multiple OCR tasks.
- Future work includes extending TrOCR for multilingual text recognition.

# References

- Key citations include Vaswani et al. (2017) for Transformers, Devlin et al. (2019) for BERT, and Dosovitskiy et al. (2021) for ViT.

# Acknowledgements

- Acknowledge contributions from team members and institutions.

# Q&A

- Invitation for questions and discussion.
