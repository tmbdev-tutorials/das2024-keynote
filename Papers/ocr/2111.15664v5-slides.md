# OCR-free Document Understanding Transformer

- Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park
- NAVER CLOVA, NAVER Search, NAVER AI Lab, Upstage, Tmax, Google, LBox
- Date of Presentation

# Introduction

- Problem: Extracting information from document images is challenging.
- Current VDU methods rely on OCR engines.
- Issues with OCR: high computational cost, inflexibility, error propagation.
- Proposal: OCR-free VDU model named Donut.

# Background/Related Work

- Conventional VDU methods use OCR for text reading.
- VDU applications: document classification, information extraction, visual question answering.
- Recent OCR-based methods show promising performance but have limitations.

# Contributions

- Propose an OCR-free approach for VDU using a Transformer architecture.
- Introduce a simple pre-training scheme with synthetic data generator SynthDoG.
- Extensive experiments show state-of-the-art performance in speed and accuracy.
- Code, pre-trained model, and synthetic data available on GitHub.

# Objective

- Main objective: Develop an effective OCR-free VDU model.
- Hypothesis: Transformer-based model can outperform OCR-dependent methods in VDU tasks.

# Methodology Overview

- Approach: Direct mapping from raw input image to desired output without OCR.
- Model: Transformer-based architecture called Donut.
- Pre-training: Cross-entropy loss for reading texts, using synthetic and real document images.

# Datasets

- IIT-CDIP: 11M scanned English document images.
- SynthDoG: Synthetic data generator producing documents in various languages.
- Other datasets: CORD, Ticket, Business Card, Receipt, DocVQA.

# Model Details

## Encoder

- Visual encoder converts document image into embeddings.
- Uses Swin Transformer for best performance.
- Input image split into non-overlapping patches, processed through Swin Transformer blocks.

## Decoder

- Textual decoder generates token sequences from embeddings.
- Uses BART as decoder architecture, initialized with pre-trained multi-lingual BART model.

# Experiments

- Evaluated on document classification, information extraction, and visual question answering.
- Metrics: Field-level F1 score, Tree Edit Distance (TED) accuracy, Average Normalized Levenshtein Similarity (ANLS).

# Results

- Donut shows state-of-the-art performance across multiple datasets.
- Significant improvements in speed and accuracy compared to OCR-based models.
- Robust performance even in low-resource scenarios.

# Performance Comparisons with Prior Work

- Donut outperforms recent OCR-dependent models in memory, time cost, and accuracy.
- Achieves higher accuracy with fewer parameters and faster inference time.

# Ablation Studies

- Evaluated different pre-training strategies, encoder backbones, and input resolutions.
- SynthDoG sufficient for document IE tasks, real images beneficial for DocVQA.
- Larger input resolution improves performance but increases computational cost.

# Visualizations

((Figure showing cross-attention maps of Donut's decoder))

# Discussion

- Key insights: Donut effectively handles complex document layouts and structures.
- Interpretation: Direct mapping approach reduces error propagation and computational cost.
- Limitations: Larger input resolutions required for high accuracy on detailed tasks.

# Conclusion

- Summary: Donut provides a novel, effective OCR-free approach for VDU tasks.
- Impact: Significant improvements in practical VDU applications.
- Future work: Enhance pre-training objectives and extend to other document understanding tasks.

# References

- Key citations: Relevant previous work, foundational papers in deep learning and OCR.

# Acknowledgements

- Acknowledgements to contributors and supporting institutions.

# Q&A

- Invitation for questions.
