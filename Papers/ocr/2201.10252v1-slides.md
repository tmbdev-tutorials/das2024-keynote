# DocEnTr: An End-to-End Document Image Enhancement Transformer
- Mohamed Ali Souibgui, Sanket Biswas, Sana Khamekhem Jemni, Yousri Kessentini, Alicia Fornés, Josep Lladós, Umapada Pal
- Computer Vision Center, Universitat Autònoma de Barcelona, Digital Research Center of Sfax, Indian Statistical Institute
- Date of Presentation

# Introduction
- Document images often suffer from various degradations
- Degradations impede recognition and processing tasks like OCR
- Need for robust preprocessing to denoise and enhance images
- Document Image Enhancement (DIE) aims to restore degraded images

# Background/Related Work
- Traditional methods: Thresholding, SVM, energy-based methods
- CNN-based approaches have improved performance but have limitations
- Transformers have shown success in NLP and are now applied to vision tasks
- Vision Transformers (ViTs) capture long-range dependencies better than CNNs

# Contributions
- Introduce DocEnTr, a pure transformer-based image enhancement model
- First model leveraging ViTs in an encoder-decoder framework for DIE
- Achieves state-of-the-art results on standard document binarization benchmarks
- Comprehensive case study demonstrating the utility of ViTs for document enhancement

# Objective
- Enhance both machine-printed and handwritten document images
- Investigate the effectiveness of the DocEnTr architecture
- Evaluate performance on standard benchmarks

# Methodology Overview
- Proposed model: Scalable auto-encoder using vision transformers
- Divides degraded image into patches
- Encoder maps patches to latent representations
- Decoder reconstructs enhanced image from encoded patches

# Datasets
- DIBCO and H-DIBCO datasets for printed and handwritten degraded document images
- Palm Leaf dataset for additional training data
- Images split into overlapped patches for training

# Model Details
## Encoder
- Divides image into patches
- Embeds patches and adds positional information
- Uses transformer blocks with self-attention and MLP layers

## Decoder
- Series of transformer blocks processing encoded tokens
- Linear layer projects tokens to pixel values
- Mean squared error loss between model output and ground truth

# Experiments
## Experimental Setup
- Evaluate on DIBCO 2011, H-DIBCO 2012, DIBCO 2017, H-DIBCO 2018 datasets
- Metrics: PSNR, F-Measure, pseudo-F-measure, DRD
- Comparison with state-of-the-art methods

# Results
## Performance Metrics
- DocEnTr outperforms related approaches on most benchmarks
- Best results on DIBCO 2011, H-DIBCO 2012, DIBCO 2017
- Competitive results on H-DIBCO 2018

## Quantitative Evaluation
| Model                  | PSNR   | FM     | Fps    | DRD   |
|------------------------|--------|--------|--------|-------|
| DocEnTr-Base{8}        | 20.81  | 94.37  | 96.15  | 1.63  |
| DocEnTr-Base{16}       | 20.11  | 93.48  | 96.12  | 1.93  |
| DocEnTr-Large{16}      | 20.62  | 94.24  | 96.71  | 1.69  |

# Discussion
- DocEnTr captures high-level global dependencies using self-attention
- Achieves superior performance in recovering degraded document images
- Qualitative results show close approximation to ground truth images

# Conclusion
- Introduced a novel transformer-based model for document image enhancement
- Demonstrated effectiveness through extensive experimentation
- Future work: Explore enhancement of other degradation types and self-supervised learning

# References
- Vaswani et al., "Attention is all you need"
- Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale"
- Pratikakis et al., "ICDAR 2017 competition on document image binarization (DIBCO 2017)"
- Jemni et al., "Enhance to read better: A multi-task adversarial network for handwritten document image enhancement"

# Acknowledgements
- Supported by the Swedish Research Council, Spanish projects, and Catalan projects
- PhD Scholarship from AGAUR and DocPRESERV project

# Q&A
- Invitation for questions
