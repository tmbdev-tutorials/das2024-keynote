# Multimodal Pre-training Based on Graph Attention Network for Document Understanding

- Zhenrong Zhang, Jiefeng Ma, Jun Du, Licheng Wang, Jianshu Zhang
- National Engineering Research Center of Speech and Language Information Processing (NERC-SLIP), University of Science and Technology of China
- IFLYTEK Research, Hefei, Anhui, China
- Date of Presentation

# Introduction

- Document intelligence aims to automate information extraction from documents
- Supports business applications like document classification, entity recognition, semantic extraction
- Challenges due to diverse formats and layouts (invoices, reports, forms)
- Multimodal modeling needed to combine textual, visual, and layout information

# Background/Related Work

- Self-supervised learning is effective for learning general data representations
- Pre-training models like BERT focus on text, neglecting image and layout
- Recent models combine textual features with images and layouts
- Existing models use individual words as inputs, which may not leverage local contexts effectively

# Contributions

- Introduced GraphDoc, a multimodal graph attention-based model for document understanding
- GraphDoc fully utilizes textual, visual, and positional information
- Introduced a graph attention layer to focus on local contexts within documents
- Achieved state-of-the-art results using only 320k unlabeled documents

# Objective

- Develop a pre-training model that effectively combines text, image, and layout information
- Improve document understanding tasks like classification, entity labeling, and semantic extraction
- Reduce computational complexity by leveraging local context information

# Methodology Overview

- Multimodal framework using text, layout, and image information
- Graph attention layer focuses on local neighborhoods within documents
- Gate fusion layer for multimodal feature fusion
- Pre-trained using Masked Sentence Modeling (MSM) task on 320k unlabeled documents

# Datasets

- PubLayNet: 360k scholarly articles for pre-training visual backbone
- RVL-CDIP: 400k scanned document images for classification
- FUNSD: 199 scanned forms for entity labeling
- SROIE: 973 receipts for receipt understanding
- CORD: 1000 receipts for receipt understanding

# Model Details

## Textual Encoder

- Encodes text and spatial layout information
- Uses pre-trained Sentence-BERT for semantic embeddings
- Layout embeddings derived from bounding box coordinates

## Visual Encoder

- Uses Swin Transformer with FPN pre-trained on PubLayNet
- Extracts visual features from document images
- Combines visual features with layout embeddings

## Gate Fusion Layer

- Fuses textual and visual features
- Acts as an information residual connection across graph attention layers

# Experiments

- Extensive experiments on FUNSD, SROIE, CORD, and RVL-CDIP datasets
- Evaluated effectiveness of multimodality, joint optimization, gate fusion layer, and graph attention network
- Compared with state-of-the-art methods

# Results

- Achieved state-of-the-art performance on multiple document understanding tasks
- Demonstrated effectiveness of graph attention layer and multimodal fusion
- Ablation studies confirmed the importance of each component

# Performance Comparisons with Prior Work

- Compared with models like LayoutLM, BROS, StructuralLM, and LayoutLMv2
- GraphDoc outperformed existing models on FUNSD, SROIE, and CORD datasets
- Achieved competitive results with significantly less pre-training data

# Conclusion

- Presented GraphDoc, a multimodal graph attention-based model for document understanding
- Fully utilized textual, visual, and layout information
- Achieved state-of-the-art performance on several tasks
- Future work includes exploring more efficient pre-training tasks and expanding dataset size

# References

- Xu et al., "LayoutLM: Pre-training of Text and Layout for Document Image Understanding," KDD, 2020
- Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," NAACL, 2019
- Vaswani et al., "Attention is All You Need," NeurIPS, 2017
- Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach," arXiv, 2019
- Reimers and Gurevych, "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks," arXiv, 2019

# Acknowledgements

- National Engineering Research Center of Speech and Language Information Processing, University of Science and Technology of China
- IFLYTEK Research, Hefei, Anhui, China

# Q&A

- Invitation for Questions
