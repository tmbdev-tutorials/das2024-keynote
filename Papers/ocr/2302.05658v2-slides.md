# DocILE Benchmark for Document Information Localization and Extraction

- Štěpán Šimsa, Milan Šulc, Michal Uřičář, Yash Patel, Ahmed Hamdi, Matěj Kocián, Matyáš Skalický, Jiří Matas, Antoine Doucet, Mickaël Coustaty, Dimosthenis Karatzas
- Rossum.ai, Czech Technical University in Prague, University of La Rochelle, Computer Vision Center
- Date of Presentation

# Introduction

- Automating information extraction from business documents
- Majority of B2B communication through semi-structured documents
- Need for structured, computer-readable data for downstream applications
- Document Information Extraction (IE) as a sub-category of Document Understanding (DU)

# Background/Related Work

- Previous work on document understanding and information extraction
- Limitations of existing datasets: small scale, specific focus (e.g., receipts), lack of localization annotations
- Need for large-scale benchmarks for practical business document IE

# Contributions

- Largest dataset for Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR)
- Annotations in 55 classes, surpassing previous datasets
- Rich set of document layouts, including zero- and few-shot cases
- Baseline evaluations of popular architectures: RoBERTa, LayoutLMv3, and DETR-based Table Transformer

# Objective

- Present the DocILE benchmark for KILE and LIR tasks
- Provide a practical benchmark for evaluating information extraction methods
- Encourage reproducibility and further research in the field

# Methodology Overview

- DocILE dataset includes 6.7k annotated business documents, 100k synthetic documents, and nearly 1M unlabeled documents
- Annotations in 55 classes for KILE and LIR tasks
- Document layouts from numerous sources, including zero- and few-shot cases

# Datasets

- Annotated set: 6,680 real business documents
- Unlabeled set: 932k real business documents
- Synthetic set: 100k documents with full task labels
- Documents from UCSF Industry Documents Library and Public Inspection Files (PIF)

# Model Details

## RoBERTa

- Text-only model using RoBERTaBASE as the backbone
- Multi-label NER formulation for KILE and LIR tasks
- Pre-trained on the DocILE dataset

## LayoutLMv3

- Multi-modal transformer architecture incorporating image, text, and layout information
- Pre-trained from scratch on the DocILE dataset
- Uses masked language modeling as the pre-training objective

# Experiments

- Baseline evaluations of RoBERTa and LayoutLMv3
- Performance compared on KILE and LIR tasks
- Evaluation metrics: F1, AP, Precision, Recall

# Results

- RoBERTaBASE+SYNTH achieved the best results among permitted models
- Synthetic pre-training improved results for both KILE and LIR
- LayoutLMv3BASE, pre-trained on another document dataset, achieved the best overall results

# Performance Comparisons with Prior Work

- Comparison with existing datasets and benchmarks
- Demonstrated need for large-scale, diverse datasets in business document IE
- Improved performance with synthetic and unsupervised pre-training

# Conclusion

- DocILE benchmark provides a large-scale, practical dataset for KILE and LIR tasks
- Encourages further research and development in business document information extraction
- Future work includes exploring different model architectures, training objectives, and addressing dataset shifts

# References

- Devlin, J., et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv (2018)
- Huang, Y., et al. LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. ACM-MM (2022)
- Liu, Y., et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv (2019)

# Acknowledgements

- Funding and support from Rossum
- Contributions from annotation team: Petra Hrdličková, Kateřina Večerková
- Support from Research Center for Informatics, Grant Agency of the Czech Technical University, Amazon Research Award, MCIN/AE/NextGenerationEU, and ELSA funded by EU

# Q&A

- Invitation for Questions
