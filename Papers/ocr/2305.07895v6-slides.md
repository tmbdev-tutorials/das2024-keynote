# Title Slide
- On the Hidden Mystery of OCR in Large Multimodal Models
- Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, Xiang Bai
- Huazhong University of Science and Technology, Microsoft Research, University of Science and Technology Beijing, Chinese Academy of Sciences, South China University of Technology
- Date of Presentation: August 14, 2024

# Introduction
- Explores effectiveness of large multimodal models in text-related visual tasks
- Evaluates models like GPT4V and Gemini on tasks including Text Recognition, Scene Text-Centric VQA, Document-Oriented VQA, Key Information Extraction, and Handwritten Mathematical Expression Recognition
- Proposes OCRBench, a comprehensive evaluation benchmark with 29 datasets
- Reveals strengths and weaknesses of models, especially in handling multilingual text, handwritten text, non-semantic text, and mathematical expressions

# Background/Related Work
- Success of large language models (LLMs) like ChatGPT and GPT-4 has spurred interest in multimodal models
- Large multimodal models (LMMs) combine pretrained visual models with LLMs
- Research includes contrastive learning and generative modeling
- Previous work shows LMMs exhibit good zero-shot OCR performance in the wild

# Contributions
- Comprehensive evaluation of LMMs on various text-related visual tasks
- Introduction of OCRBench, a benchmark with 29 datasets for OCR capabilities
- Identification of LMMs' strengths and weaknesses in specific OCR tasks
- Baseline results for enhancing zero-shot multimodal techniques

# Objective
- Evaluate OCR capabilities of LMMs on five representative tasks
- Provide a foundational framework for future strategies to enhance OCR in LMMs

# Methodology Overview
- Evaluated 14 LMMs on five tasks: Text Recognition, Scene Text-Centric VQA, Document-Oriented VQA, Key Information Extraction, and Handwritten Mathematical Expression Recognition
- Used datasets from OCRBench for comprehensive assessment
- Focused on zero-shot scenarios to test generalization abilities

# Datasets
- OCRBench includes 29 datasets for comprehensive evaluation
- Tasks covered: Regular and Irregular Text Recognition, Artistic Text Recognition, Handwritten Text Recognition, Chinese Text Recognition, Handwritten Digit String Recognition, Non-Semantic Text Recognition
- Scene Text-Centric VQA datasets: STVQA, TextVQA, OCRVQA, ESTVQA
- Document-Oriented VQA datasets: DocVQA, InfographicVQA, ChartQA
- KIE datasets: SROIE, FUNSD, POIE
- HMER dataset: HME100K

# Model Details
- Evaluated models include GPT4V, Gemini, BLIP2, mPLUG-Owl, InstructBLIP, LLaVAR, BLIVA, mPLUG-Owl2, LLaVA1.5, UniDoc, Docpedia, Monkey
- Models vary in input resolution and architecture
- Training procedures involve both supervised and unsupervised methods

# Experiments
- Evaluation metric: presence of ground truth in LMM output
- Filtered out questions with answers containing fewer than 4 symbols
- Tested on 3000 question instances from large datasets

# Results
- LMMs show comparable performance to state-of-the-art models in regular, irregular, occluded, and artistic text recognition
- Poor performance in handwritten text, Chinese text, handwritten digit strings, and non-semantic text recognition
- Larger input resolutions improve performance in complex tasks like Document-Oriented VQA and KIE

# Performance Comparisons with Prior Work
- LMMs perform well in regular and irregular text recognition but lag behind domain-specific methods in handwritten and multilingual text recognition
- Comparison with state-of-the-art supervised models shows significant gaps in certain tasks

# Ablation Studies
- Semantic reliance: LMMs depend heavily on semantic understanding for word recognition
- Handwritten text: Challenges due to shape similarities and low-quality images
- Multilingual text: Notable performance gap in Chinese text recognition
- Fine-grain perception: Higher resolutions needed for detailed information extraction

# Discussion
- Key insights: LMMs rely on semantic understanding, face difficulties with handwritten and multilingual text
- Interpretation of results: Limited by training data and input resolution
- Limitations: Need for higher resolution and more multilingual training data

# Conclusion
- LMMs show promising results in OCR tasks, especially in regular text recognition
- Significant gaps remain compared to domain-specific methods
- OCRBench serves as a valuable benchmark for evaluating and improving LMM OCR capabilities
- Future work: Explore potential in more scenarios, complex tasks, and multiple languages

# References
- Key citations include works on LLMs, multimodal models, and OCR benchmarks

# Acknowledgements
- Supported by National Natural Science Foundation of China
- Thanks to Hongliang Li, Yang Liu, Dezhi Peng, Mingyu Liu, and Mingrui Chen

# Q&A
- Invitation for questions
