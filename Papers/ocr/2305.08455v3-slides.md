# Document Understanding Dataset and Evaluation (DUDE)

- Jordy Van Landeghem, Rubèn Tito, Łukasz Borchmann, Michał Pietruszka, Paweł Józiak, Rafał Powalski, Dawid Jurkiewicz, Mickaël Coustaty, Bertrand Ackaert, Ernest Valveny, Matthew Blaschko, Sien Moens, Tomasz Stanisławek
- KU Leuven, Contract.fit, Snowflake, Warsaw University of Technology, Computer Vision Center, Universitat Autònoma de Barcelona, Jagiellonian University, Adam Mickiewicz University, Instabase, University of La Rochelle

# Introduction

- Challenge of creating more practical benchmarks in Document AI
- DUDE addresses the halted research progress in understanding visually-rich documents
- New dataset with diverse types of questions, answers, and document layouts
- Aims to simulate real-world scenarios for better generalization and adaptation

# Background/Related Work

- Existing benchmarks focus on single domains, limiting generalizability
- Visual Question Answering (VQA) as an interface for document understanding
- Prior VQA datasets focus on single-page documents with extractive questions
- DUDE introduces multi-page, multi-domain documents with diverse question types

# Contributions

- Creation of DUDE: a large-scale, multi-paged, multi-domain DocVQA benchmark
- Evaluates zero-shot and fine-tuned performance of current state-of-the-art models
- Highlights the gap between model performance and human baselines
- Pushes for more holistic and efficient modeling of language, vision, and layout

# Objective

- Evaluate current DU solutions on multi-page documents
- Test models' ability to navigate and reason over visual layouts
- Assess generalization to different document types and domains

# Methodology Overview

- High-level approach: Document Visual Question Answering (DocVQA)
- Data collected from diverse sources and domains
- Evaluation setup targets models answering diverse and natural questions

# Datasets

- Covers various document types, sources, and dates
- Includes medical, legal, technical, and financial domains
- Documents feature diverse layouts, text arrangements, font sizes, and styles

# Model Details

- Incorporates text, layout, and visual elements
- Evaluated both zero-shot and fine-tuned models
- Models include BERT, Longformer, T5, GPT-3, and Hi-VT5

# Experiments

- Evaluation metrics: Average Normalized Levenshtein Similarity (ANLS), Expected Calibration Error (ECE), Area-Under-Risk-Coverage-Curve (AURC)
- Baselines include both encoder-only and encoder-decoder models
- Analysis of model performance on different question types and document elements

# Results

- Human baseline ANLS: 74.76
- Best-performing model: T5-2D-large-8K with 46.04 ANLS
- Significant gap between model and human performance
- Challenges in handling visual elements and long documents

# Performance Comparisons with Prior Work

- DUDE outperforms prior VQA datasets in diversity and complexity
- Models struggle with visual evidence and multi-hop questions
- Long document processing remains a significant challenge

# Discussion

- Key insights: Importance of visual understanding, handling diverse layouts, and processing long documents
- Limitations: Focus on English documents, dataset size relative to real-world diversity
- Need for better integration of visual modalities and layout comprehension

# Conclusion

- DUDE sets a new standard for practical benchmarks in Document AI
- Highlights the gap between current models and human performance
- Future work should focus on better visual understanding and handling of diverse document types

# References

- Biten et al., "Scene Text Visual Question Answering," 2019
- Mathew et al., "Document Visual Question Answering Challenge 2020," 2020
- Yang et al., "On Multi-Domain Long-Tailed Recognition," 2022
- Rajpurkar et al., "Know What You Don’t Know: Unanswerable Questions for SQuAD," 2018
- Geifman and El-Yaniv, "Selective Classification for Deep Neural Networks," 2017

# Acknowledgements

- Funding and support from respective institutions and organizations

# Q&A

- Invitation for questions and further discussion
