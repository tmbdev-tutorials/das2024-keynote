# KOSMOS-2: Grounding Multimodal Large Language Models to the World

- Authors: Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei
- Institution: Microsoft Research
- Date of Presentation: [Insert Date]

# Introduction

- Multimodal Large Language Models (MLLMs) are versatile across language, vision, and vision-language tasks.
- Existing MLLMs lack grounding capabilities, limiting human-AI interaction efficiency.
- KOSMOS-2 introduces grounding capability, linking text spans to visual regions via bounding boxes.
- Evaluation on tasks like multimodal grounding, referring expression comprehension, and generation.

# Background/Related Work

- Previous MLLMs like HSD+22, ADL+22, HDW+23, and DXS+23.
- KOSMOS-2 builds on KOSMOS-1, adding grounding capability.
- Grounding capability enables linking text spans to specific image regions.

# Contributions

- Introduction of KOSMOS-2, a grounded MLLM.
- Creation of GRIT, a large-scale dataset of grounded image-text pairs.
- Enhanced performance on grounding tasks and multimodal referring tasks.
- Lays the foundation for Embodiment AI and convergence of language, perception, action, and world modeling.

# Objective

- Main Objective: To integrate grounding capability into MLLMs.
- Research Questions: How can grounding improve human-AI interaction? How does grounding affect model performance on various tasks?

# Methodology Overview

- High-Level Approach: Use grounded image-text pairs to train KOSMOS-2.
- Data Used: GRIT dataset, combining subsets of LAION-2B and COYO-700M.
- Model Architecture: Transformer-based causal language model with grounding capability.

# Datasets

- GRIT Dataset: Grounded Image-Text pairs from LAION-2B and COYO-700M.
- Data Preprocessing: Extract and link noun phrases to image regions using bounding boxes.
- Data Format: Represent bounding boxes as sequences of location tokens appended to text spans.

# Model Details

- Model Architecture: Transformer-based causal language model.
- Key Components: Vision encoder, resampler module, and location tokens.
- Training Procedure: Next-word prediction task using grounded image-text pairs.

# Experiments

- Experimental Setup: Evaluation on multimodal grounding, referring expression comprehension, and generation tasks.
- Evaluation Metrics: Recall (R@1, R@5, R@10) for grounding tasks, METEOR and CIDEr for referring expression generation.

# Results

- Grounding Tasks: Impressive zero-shot performance on phrase grounding and referring expression comprehension.
- Referring Expression Generation: Competitive performance, showcasing in-context learning ability.
- Perception-Language Tasks: Comparable performance to KOSMOS-1 on image captioning and visual question answering.

# Performance Comparisons with Prior Work

- Comparison with Baselines: Outperforms GRILL on phrase grounding without relying on object queries or proposals.
- Comparison with State-of-the-Art: Competitive performance on various tasks, demonstrating the effectiveness of grounding.

# Discussion

- Key Insights: Grounding capability enhances human-AI interaction and provides more accurate responses.
- Interpretation of Results: Grounding allows the model to link text and visual regions effectively.
- Limitations: Slight performance decrease on some language tasks, future work needed to balance capabilities.

# Conclusion

- Summary of Contributions: Introduction of KOSMOS-2 with grounding capability, creation of GRIT dataset.
- Impact of Findings: Grounding as a foundational capability for MLLMs, improving interaction and performance.
- Future Work Directions: Enhance understanding of human expressions, expand grounding capabilities.

# References

- Key Citations: HSD+22, ADL+22, HDW+23, DXS+23, SBV+22, BPK+22, and more.

# Acknowledgements

- Some examples taken from the WHOOPS corpus.
- Microsoft AI Principles applied in model development.

# Q&A

- Invitation for Questions
