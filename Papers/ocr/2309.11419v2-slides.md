# KOSMOS-2.5: A Multimodal Literate Model

- Tengchao Lv, Yupan Huang, Jingye Chen, Yuzhong Zhao, Yilin Jia, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei
- Microsoft
- Date of Presentation: August 21, 2024

# Introduction

- Multimodal large language models (MLLMs) extend LLMs to multimodal tasks
- Existing MLLMs focus on natural images, not text-intensive images
- Traditional OCR methods preserve layout but miss document-level reading order
- Markdown-formatted text distinguishes structural elements

# Background/Related Work

- Existing approaches limited to line-level text recognition or specific document categories
- OCR methods neglect document-level structural integrity
- Markdown offers advantages by distinguishing structural elements

# Contributions

- Introduce KOSMOS-2.5 for text-intensive document understanding
- Unified model for spatially-aware text blocks and markdown generation
- Fine-tuned KOSMOS-2.5-CHAT for document understanding tasks
- Curated a large corpus of 357.4 million document pages

# Objective

- Develop a model capable of reading and understanding text-intensive documents
- Achieve high performance in document-level text recognition and image-to-markdown generation
- Evaluate model on newly proposed benchmarks: OCREval and MarkdownEval

# Methodology Overview

- Pre-trained on a large corpus of text-intensive images
- Two complementary transcription tasks: spatially-aware text blocks and structured markdown output
- Shared Transformer architecture with task-specific prompts
- Fine-tuned for document understanding tasks

# Datasets

- Large corpus of 357.4 million document pages
- Includes scanned documents, general PDFs, SEC files, arXiv papers, web pages, design images, handwritten texts, mathematical content, project documents
- Documents annotated with text lines, bounding boxes, or markdown formats

# Model Details

## Model Architecture

- Vision encoder based on Vision Transformer (ViT)
- Language decoder using Transformer architecture
- Resampler module connects encoder and decoder to reduce sequence length
- ((Figure showing model architecture))

## Training Procedure

- Pre-training on document-level text recognition and image-to-markdown generation
- Fine-tuning on document understanding tasks with frozen visual encoder
- Task-specific prompts for different tasks

# Experiments

- Evaluated on OCREval and MarkdownEval benchmarks
- OCREval: 2,297 samples for document-level text recognition
- MarkdownEval: 5,693 samples for image-to-markdown generation
- Metrics: F1, IOU, NED for OCREval; NED, NTED for MarkdownEval

# Results

- KOSMOS-2.5 outperforms existing models in document-level text recognition
- Achieves best performance across all image types on MarkdownEval
- Comparable to state-of-the-art models with fewer parameters

# Performance Comparisons with Prior Work

- KOSMOS-2.5 surpasses VaryBase despite smaller model size (1.3B vs. 7B parameters)
- Outperforms models adhering to markdown standards (e.g., Vary, Nougat)
- Better layout understanding in text recognition

# Conclusion

- KOSMOS-2.5 advances document-level machine reading
- Unified framework for spatially-aware text blocks and markdown generation
- Extensive corpus and benchmarks for future research
- Future work: handling documents spanning multiple pages, scaling model capabilities

# References

- Key citations related to multimodal large language models, document reading, and understanding

# Q&A

- Invitation for questions
