# Title Slide

- TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document
- Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, Xiang Bai
- Huazhong University of Science and Technology, Kingsoft
- Date of Presentation: 2024

# Introduction

- Extracting key information from diverse sources is crucial for automation
- Early methods use a two-stage approach leading to error accumulation
- OCR-Free solutions attract attention to alleviate external system drawbacks
- Large multimodal models (LMMs) face limitations in text-related tasks

# Background/Related Work

- OCR-Model-Driven methods rely on OCR tools for text extraction
- OCR-Free methods do not require off-the-shelf OCR engines
- Previous methods include StrucTexT, ERNIE-Layout, LayoutLM, UDOP, and Donut
- Recent advancements in MLLMs like LLaVAR, UniDoc, TGDoc, mPLUG-DocOwl

# Contributions

- TextMonkey introduces Shifted Window Attention with zero-initialization
- Token resampler to compress redundant tokens and enhance performance
- Expands capabilities to text spotting and grounding with positional information
- Demonstrates significant improvements across 12 benchmarks

# Objective

- Develop a large multimodal model tailored for text-centric tasks
- Enhance model performance by addressing cross-window connectivity
- Reduce token redundancy while maintaining important features
- Improve interpretability and support text grounding tasks

# Methodology Overview

- Split Module divides high-resolution images into window patches
- Shifted Window Attention builds cross-window connections
- Token resampler compresses redundant tokens using similarity
- Incorporate positional information to enhance model reliability

# Datasets

- Scene Text: COCOText, TextOCR, HierText, TextVQA, MLT
- Document: IIT-CDIP, DocVQA, ChartQA, InfoVQA, DeepForm, Kleister Charity, WikiTableQuestions
- Training data includes 409.1k dialogue pairs and 2.1M question-answer pairs

# Model Details

- Model Architecture: Transformer blocks inherited from pre-trained CLIP
- Image resampler with 256 learnable queries
- Token resampler with a ratio set to 512 for 896 resolution and 1024 for 1344 resolution
- Overall, TextMonkey has 9.7B parameters

# Experiments

- Evaluation metrics include accuracy, F1 score, ANLS, and VQA score
- Comparison with recent large multimodal models on benchmarks
- OCRBench, Document Benchmarks, and Text Spotting datasets used for evaluation

# Results

- Significant performance improvements: 5.2% in Scene Text-Centric VQA, 6.9% in Document-Oriented VQA, 2.8% in KIE tasks
- Achieved a score of 561 on OCRBench, surpassing previous models
- Demonstrated effectiveness in text-related tasks with high accuracy

# Performance Comparisons with Prior Work

- Compared with models like BLIP2, mPLUG-Owl, InstructBLIP, LLaVAR, BLIVA, and others
- TextMonkey outperformed in both Scene Text-Centric and Document-Oriented tasks
- Notable improvements in benchmarks like DocVQA, ChartQA, and TextVQA

# Ablation Studies

- Zero initialization improves performance by 0.6% on ChartQA
- Cross-window connections and token resampler enhance understanding and reduce redundancy
- Detailed comparison of token length reduction strategies

# Visualizations

- Visual results show accurate text localization and identification in various scenarios
- Examples include natural scenes, documents, charts, and tables
- Highlighted the model's capability to perceive and comprehend textual information

# Discussion

- Examined grounding information to understand model errors
- Incorporating position improves performance in datasets like DocVQA and SROIE
- Chain-of-thought prompting as a future research direction
- Comparison between different positional representations: points, rectangles, polygons

# Conclusion

- TextMonkey addresses challenges in text-centric tasks with innovative techniques
- Shifted Window Attention and Token Resampler enhance model performance
- Demonstrated excellent results on multiple benchmarks
- Future work includes optimizing resolution scaling and exploring chain-of-thought automation

# References

- Key citations include works on OCR-Free methods, large multimodal models, and document understanding benchmarks

# Acknowledgements

- Acknowledge contributions from team members and supporting institutions

# Q&A

- Invitation for questions and further discussion
