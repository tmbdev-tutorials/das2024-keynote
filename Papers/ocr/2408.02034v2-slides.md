# Mini-Monkey: Multi-Scale Adaptive Cropping for Multimodal Large Language Models

- Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, Xiang Bai
- Huazhong University of Science and Technology, South China University of Technology
- Presentation Date: [Insert Date]

# Introduction

- Enhancement of multimodal large language models (MLLMs) to process high-resolution images
- Existing cropping methods impair recognition of small or irregularly shaped objects
- Proposal of Mini-Monkey to address these issues
- Incorporation of Multi-Scale Adaptive Cropping Strategy (MSAC) and Scale Compression Mechanism (SCM)

# Background/Related Work

- Development of Large Language Models (LLMs) and their extension to multimodal models
- Existing methods: Flamingo, BLIP-2, MiniGPT4, Qwen-VL, LLaVA, CogVLM
- Common approach: Cropping strategy to handle high-resolution images
- Issues with current methods: Sawtooth effect and semantic incoherence

# Contributions

- Introduction of Mini-Monkey, a lightweight MLLM
- Multi-Scale Adaptive Cropping Strategy (MSAC) to mitigate semantic incoherence
- Scale Compression Mechanism (SCM) to reduce computational overhead
- State-of-the-art performance on various multimodal understanding tasks

# Objective

- Main objective: Enhance MLLMs' ability to process high-resolution images without segmenting objects
- Hypothesis: Multi-scale representations and compression mechanisms improve performance and efficiency

# Methodology Overview

- Mini-Monkey architecture: MSAC, vision encoder, MLP layer, SCM, and LLM
- Adaptive generation of multi-scale representations
- Compression of image tokens to mitigate computational overhead

# Datasets

- Training datasets: DocVQA, ChartQA, DVQA, AI2D, GeoQA+, LLaVA-150K (zh)
- Evaluation benchmarks: MathVista, SEED Image, RealWorldQA, AI2D test, POPE, CCBench, MME, HallusionBench

# Model Details

## Multi-Scale Adaptive Cropping Strategy

- Generates a pre-defined set of grids
- Stratified operation on grids based on aspect ratios
- Adaptive selection of multiple aspect ratios to avoid semantic incoherence
- Fusion of multi-scale visual representations within the LLM

## Scale Compression Mechanism

- Parameter-free token compression method
- Focuses on compressing tokens from detailed layers
- Utilizes LLM's attention layers to select necessary visual features

# Experiments

- Evaluation on 11 general multimodal understanding benchmarks
- Metrics: Performance on MathVista, POPE, HallusionBench, and others

# Results

- State-of-the-art performance among 2B-parameter MLLMs
- Outperforms previous methods by an average of 1.7% across 13 benchmarks
- Score of 802 on OCRBench, surpassing 8B-parameter model InternVL2-8B

# Performance Comparisons with Prior Work

- Comparison with baselines and state-of-the-art models
- Significant improvements in document understanding capabilities
- Efficient training requiring only eight RTX 3090 GPUs

# Ablation Studies

- Effectiveness of MSAC and SCM demonstrated through ablation studies
- Comparison with dynamic high-resolution, fixed-size high-resolution, and overlapping cropping strategies
- Integration of MSAC into different MLLM architectures showing consistent performance enhancement

# Conclusion

- Summary of Mini-Monkey's contributions and performance
- Impact on multimodal large language model capabilities
- Future work: Further optimization and exploration of additional applications

# References

- Key citations from the paper's bibliography

# Q&A

- Invitation for questions
