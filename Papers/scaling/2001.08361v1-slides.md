# Scaling Laws for Neural Language Models

- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei
- Johns Hopkins University, OpenAI
- Date of Presentation

# Introduction

- Study of empirical scaling laws for language model performance on cross-entropy loss
- Performance scales as a power-law with model size, dataset size, and compute used for training
- Architectural details like network width or depth have minimal effects within a wide range
- Simple equations govern overfitting and training speed dependence on model size
- Larger models are significantly more sample-efficient

# Background/Related Work

- Language modeling has seen rapid progress with state-of-the-art models approaching human-level performance
- Performance depends on model architecture, size, computing power, and data available for training
- This work focuses on the Transformer architecture and studies trends over seven orders of magnitude in scale

# Contributions

- Empirical investigation of language modeling loss dependence on model size, dataset size, and compute
- Identification of power-law relationships for performance
- Determination of optimal compute allocation
- Demonstration that larger models are more sample-efficient

# Objective

- Investigate scaling laws for language model performance
- Understand the dependence of language modeling loss on various factors
- Provide a predictive framework for optimal compute allocation

# Methodology Overview

- Train language models on WebText2 dataset using byte-pair encoding
- Optimize autoregressive log-likelihood averaged over a 1024-token context
- Use Adam optimizer for most models and Adafactor for largest models

# Datasets

- WebText2: 20.3M documents, 96 GB of text, 1.62 x 10^10 words
- Tokenized using byte-pair encoding, resulting in 2.29 x 10^10 tokens
- Test set includes samples from Books Corpus, Common Crawl, English Wikipedia, and Internet Books

# Model Details

- Transformer architecture with hyperparameters: nlayer, dmodel, dff, dattn, nheads
- Model size N defined as the number of non-embedding parameters
- Training involves approximately 6N floating point operations per token

# Empirical Results and Basic Power Laws

- Performance depends strongly on scale factors: model parameters (N), dataset size (D), compute (C)
- Power-law relationships observed for each factor when not bottlenecked by the other two
- Performance penalty predictable based on the ratio N^0.74/D

# Scaling Laws with Model Size and Training Time

- Training curves follow predictable power-laws independent of model size
- Early training performance can predict long-term outcomes
- Optimal performance involves training very large models with relatively modest data and stopping before convergence

# Optimal Allocation of the Compute Budget

- Larger models should be trained with a smaller amount of data and stopped before full convergence
- Optimal model size grows rapidly with compute budget
- Ideal batch size determined by the gradient noise scale

# Discussion

- Language modeling performance improves predictably with increased model size, data, and compute
- Larger models are more sample-efficient and perform better
- Future work may explore the generalization of these scaling laws to other generative modeling tasks

# Conclusion

- Key findings include power-law relationships for language model performance
- Optimal compute-efficient training involves training large models with modest data
- Larger models are more sample-efficient, suggesting further improvements with larger models

# References

- Advani & Saxe, 2017
- Belkin et al., 2018
- Child et al., 2019
- Hestness et al., 2017
- Tan & Le, 2019

# Acknowledgements

- Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner, Danny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, Ilya Sutskever

# Q&A

- Invitation for Questions
