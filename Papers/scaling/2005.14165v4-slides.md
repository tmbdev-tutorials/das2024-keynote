# Language Models are Few-Shot Learners

- Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah
- Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam
- Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss
- Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh
- Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse
- Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray
- Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish
- Alec Radford, Ilya Sutskever, Dario Amodei
- OpenAI
- Date of Presentation: July 22, 2020

# Introduction

- Substantial gains in NLP tasks by pre-training on large text corpora.
- Current methods require task-specific fine-tuning datasets.
- Humans perform new language tasks from few examples or instructions.
- This paper explores scaling up language models for improved few-shot performance.

# Background/Related Work

- Previous work on pre-trained language representations like word vectors and RNNs.
- Recent progress with transformer language models fine-tuned for specific tasks.
- Limitations of task-specific fine-tuning and the need for large datasets.
- Concept of meta-learning and in-context learning.

# Contributions

- Training GPT-3, a 175 billion parameter autoregressive language model.
- Demonstrating GPT-3's task-agnostic, few-shot performance on various NLP tasks.
- Identifying strengths and weaknesses of GPT-3 in few-shot learning settings.
- Discussing broader societal impacts of GPT-3.

# Objective

- Evaluate GPT-3 on over two dozen NLP datasets.
- Test GPT-3's performance in zero-shot, one-shot, and few-shot settings.
- Compare GPT-3's performance with state-of-the-art fine-tuned models.
- Examine GPT-3's ability to generate human-like text.

# Methodology Overview

- Scaling up model size, dataset size, and training length.
- Using in-context learning for task specification.
- Evaluating GPT-3 on tasks without gradient updates or fine-tuning.
- Systematic study of different settings for learning within the context.

# Datasets

- CommonCrawl (filtered), WebText2, Books1, Books2, Wikipedia.
- Filtering based on similarity to high-quality reference corpora.
- Fuzzy deduplication to prevent redundancy.
- Sampling datasets in proportion to perceived quality.

# Model Details

- GPT-3 architecture similar to GPT-2 with modifications like alternating dense and sparse attention patterns.
- Training eight different model sizes ranging from 125 million to 175 billion parameters.
- Using a context window of 2048 tokens for all models.
- Training with Adam optimizer and cosine decay for learning rate.

# Experiments

- Evaluating GPT-3 on various NLP tasks including language modeling, question answering, translation, and reasoning.
- Testing in zero-shot, one-shot, and few-shot settings.
- Using beam search for tasks with free-form completion.

# Results

- GPT-3 shows strong performance in few-shot settings, sometimes competitive with fine-tuned models.
- Examples of tasks include LAMBADA, HellaSwag, StoryCloze, TriviaQA, and WebQuestions.
- Few-shot learning improves significantly with model size.

# Performance Comparisons with Prior Work

- GPT-3's few-shot performance approaches or exceeds state-of-the-art fine-tuned models on some tasks.
- Performance gains observed across various NLP benchmarks.
- Detailed comparisons on tasks like TriviaQA, WebQuestions, and Natural Questions.

# Visualizations

- In-context learning curves showing task performance as a function of the number of examples.
- Performance trends with model size and number of examples in the context.
- Aggregated results for different model sizes and settings.

# Discussion

- Key insights into GPT-3's strengths and limitations.
- Interpretation of results and areas where GPT-3 struggles.
- Consideration of methodological issues related to training on large web corpora.

# Conclusion

- GPT-3 demonstrates promising few-shot learning capabilities.
- Potential for very large language models to develop adaptable, general language systems.
- Future work directions including improving sample efficiency and addressing biases.

# References

- Vaswani et al. (2017), "Attention is All You Need"
- Devlin et al. (2018), "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
- Radford et al. (2019), "Language Models are Unsupervised Multitask Learners"
- Raffel et al. (2019), "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"

# Acknowledgements

- Thanks to Ryan Lowe, Jakub Pachocki, Szymon Sidor, Greg Brockman, Michael Petrov, Brooke Chan, Chelsea Voss, David Luan, Irene Solaiman, Harrison Edwards, Yura Burda, Geoffrey Irving, Paul Christiano, Long Ouyang, Chris Hallacy, and Shan Carter for their contributions.

# Q&A

- Invitation for Questions
