# Measuring Massive Multitask Language Understanding

- Dan Hendrycks (UC Berkeley)
- Collin Burns (Columbia University)
- Steven Basart (UChicago)
- Andy Zou (UC Berkeley)
- Mantas Mazeika (UIUC)
- Dawn Song (UC Berkeley)
- Jacob Steinhardt (UC Berkeley)
- ICLR 2021

# Introduction

- NLP models excel on recent benchmarks but still fall short in overall language understanding
- General Language Understanding Evaluation (GLUE) and SuperGLUE benchmarks
- Need for a benchmark that evaluates models across diverse subjects and difficulty levels
- Proposed new benchmark to measure academic and professional understanding

# Background/Related Work

- Current benchmarks (GLUE, SuperGLUE) focus on linguistic skills
- Transformer models pretrain on massive text corpora
- Prior benchmarks don't assess specialized domain knowledge
- Few-shot learning enables evaluation without extensive fine-tuning

# Contributions

- Introduction of a new benchmark covering 57 diverse subjects
- Evaluation of models exclusively in zero-shot and few-shot settings
- Analysis of model performance across a wide range of academic and professional tasks
- Identification of model shortcomings in specific domains

# Objective

- Assess the breadth and depth of models' academic and professional understanding
- Measure knowledge acquired during pretraining
- Evaluate models' problem-solving abilities in various domains

# Methodology Overview

- Benchmark consists of multiple-choice questions from 57 subjects
- Subjects span STEM, humanities, social sciences, and more
- Questions manually collected from freely available sources
- Evaluation in zero-shot and few-shot settings

# Datasets

- 15908 questions in total
- Few-shot development set: 5 questions per subject
- Validation set: 1540 questions
- Test set: 14079 questions
- Questions sourced from practice exams and academic resources

# Model Details

- Models evaluated: GPT-3, UnifiedQA, RoBERTa-base, ALBERT-xxlarge, GPT-2
- GPT-3 variants: Small (2.7B parameters), Medium (6.7B), Large (13B), X-Large (175B)
- UnifiedQA uses T5 backbone, fine-tuned on QA datasets
- Few-shot learning setup with up to 5 examples per prompt

# Experiments

- Performance measured by classification accuracy across all tasks
- Comparison of GPT-3 and UnifiedQA performance
- Analysis of model calibration and confidence

# Results

- GPT-3 X-Large achieves 43.9% accuracy, UnifiedQA 48.9%
- Smaller GPT-3 models have near-random accuracy (~25%)
- UnifiedQA benefits from fine-tuning on QA datasets
- Performance varies significantly across subjects

# Performance Comparisons with Prior Work

- GPT-3 performs better than random on new benchmark but has lopsided performance
- UnifiedQA outperforms GPT-3 despite fewer parameters
- Both models struggle with calculation-heavy and procedural tasks

# Visualizations

- ((Figure showing GPT-3 and UnifiedQA accuracy across 57 tasks))
- ((Figure showing GPT-3 confidence vs. accuracy))

# Discussion

- Models have substantial knowledge gaps and lopsided performance
- Difficulty with procedural and calculation-heavy tasks
- Calibration issues: confidence not a reliable estimator of accuracy
- Need for improved pretraining and evaluation methodologies

# Conclusion

- New benchmark assesses a wide range of academic and professional knowledge
- Current models show meaningful progress but still fall short of expert-level performance
- Future work should focus on addressing model shortcomings in specific domains
- Benchmark provides a clearer picture of state-of-the-art capabilities

# References

- Wang et al., 2018 (GLUE)
- Wang et al., 2019 (SuperGLUE)
- Brown et al., 2020 (GPT-3)
- Hendrycks et al., 2020 (ETHICS)
- Raffel et al., 2019 (T5)

# Acknowledgements

- Oyvind Tafjord, Jan Leike, David Krueger, Alex Tamkin, Girish Sastry, Henry Zhu
- Supported by NSF GRFP Fellowship, Open Philanthropy Project Fellowship
- NSF Frontier Award 1804794

# Q&A

- Invitation for Questions
