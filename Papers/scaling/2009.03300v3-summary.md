# ONE SENTENCE SUMMARY:
The paper introduces a comprehensive benchmark to evaluate NLP models across 57 diverse tasks, revealing significant knowledge gaps and highlighting the need for improvements in model accuracy and calibration.

# MAIN POINTS:
1. A new benchmark evaluates text models on 57 tasks, including STEM, humanities, and social sciences.
2. High accuracy requires models to have extensive world knowledge and problem-solving ability.
3. Recent models perform near random-chance accuracy; GPT-3 shows a 20% improvement on average.
4. Models exhibit lopsided performance and often lack awareness of their errors.
5. GPT-3's accuracy varies widely across tasks, excelling in some but failing in others.
6. Tasks include elementary to advanced levels, from traditional subjects to specialized areas.
7. The benchmark assesses models in zero-shot and few-shot settings, akin to human evaluation.
8. Human performance on the test varies, with expert-level accuracy around 89.8%.
9. GPT-3 shows better results with increased model size but still lacks expert-level performance.
10. Calibration of model confidence remains a significant issue, with discrepancies up to 24%.

# TAKEAWAYS:
1. Comprehensive benchmarks like this one are crucial for identifying NLP model shortcomings.
2. Current state-of-the-art models need substantial improvements to achieve human-level accuracy.
3. Models struggle with procedural knowledge and tasks related to human values, such as law and morality.
4. Fine-tuning on specialized datasets can help but may not be sufficient for significant accuracy improvements.
5. Future NLP evaluations should consider multimodal inputs and more extensive pretraining data.
