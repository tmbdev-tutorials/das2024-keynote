# A Systematic Investigation of Commonsense Knowledge in Large Language Models
- Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d’Autume, Phil Blunsom, Aida Nematzadeh
- Affiliations: Allen Institute for Artificial Intelligence, DeepMind, Inflection AI, Reka, Cohere, University of Oxford
- Date of Presentation: December 7-11, 2022

# Introduction
- Examine the extent to which large language models (LMs) learn commonsense knowledge
- Focus on zero-shot and few-shot commonsense evaluations
- Control for potential surface cues and annotation artifacts
- Investigate performance variations not related to commonsense knowledge

# Background/Related Work
- Previous work shows impressive performance on NLP tasks by LMs (e.g., Brown et al., 2020; Patwary et al., 2021)
- Commonsense knowledge is critical for NLP applications
- Previous benchmarks show limitations in evaluating commonsense without task-specific supervision

# Contributions
- Conduct a systematic and rigorous evaluation of large pre-trained LMs on commonsense benchmarks
- Control for LMs' ability to exploit surface cues and annotation artifacts
- Highlight limitations in achieving human-level commonsense performance without task-specific supervision

# Objective
- Understand how well pre-trained LMs capture commonsense knowledge
- Examine zero-shot and few-shot performance on commonsense benchmarks
- Evaluate the impact of model size and evaluation design choices on performance

# Methodology Overview
- Evaluate LMs on four commonsense benchmarks: HellaSwag, WinoGrande, Social IQa, PIQA
- Use six model sizes, up to 280B parameters
- Conduct experiments with different score functions and prompt formats

# Datasets
- HellaSwag: Physical, grounded, and temporal commonsense
- WinoGrande: Physical and social commonsense
- Social IQa: Social commonsense, theory of mind
- PIQA: Physical commonsense

## Dataset Statistics
| Benchmark     | Choices | Knowledge Types       | Questions |
|---------------|---------|-----------------------|-----------|
| HellaSwag     | 4       | Temporal, Physical    | 10042     |
| WinoGrande    | 2       | Social, Physical      | 1267      |
| Social IQa    | 3       | Social                | 1954      |
| PIQA          | 2       | Physical              | 1838      |

# Model Details
- Pre-trained LM: Gopher (280B parameters)
- Model sizes: 44M, 117M, 417M, 1.4B, 7.1B, 280B
- Training data: MassiveText dataset with 2 trillion tokens from various domains

# Experiments
- Zero-shot and few-shot evaluations
- Score functions: Cross-entropy, sequence log probability, point-wise mutual information
- Prompt formats: Question-answer concatenation, special symbols, sentence conversion

# Results
- Zero-shot performance improves with larger model sizes
- Few-shot evaluation yields small improvements, except for Social IQa
- Evaluation design choices significantly impact performance

# Performance Comparisons with Prior Work
- Zero-shot performance shows a gap with state-of-the-art results
- Larger models better exploit surface cues and annotation artifacts
- Human-level performance requires infeasibly large models (e.g., >100T parameters)

# Discussion
- Large LMs show limited commonsense reasoning without task-specific supervision
- Evaluation design choices play a crucial role in reported performance
- Need for standardized evaluation protocols and robust performance metrics

# Conclusion
- Large LMs have limitations in acquiring commonsense knowledge
- Increasing model size alone is insufficient to achieve human-level performance
- Future work should explore explicit commonsense supervision, multi-modal grounding, and physical embodiment

# References
- Brown et al., 2020
- Patwary et al., 2021
- Zellers et al., 2019a
- Sakaguchi et al., 2020
- Bisk et al., 2020b

# Acknowledgements
- Thanks to Ivana Kajić, Laura Rimell, Stella Biderman, and anonymous reviewers for their feedback
- Thanks to Jack W. Rae and Gopher paper authors for providing evaluation pipelines

# Q&A
- Invitation for Questions
