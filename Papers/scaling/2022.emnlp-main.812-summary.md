# ONE SENTENCE SUMMARY:
This study systematically evaluates large language models' (LMs) ability to understand commonsense knowledge in zero-shot and few-shot settings, revealing limitations and the impact of evaluation design choices.

# MAIN POINTS:
1. Large language models show impressive zero-shot performance on various NLP tasks.
2. The study evaluates LMs' acquisition of commonsense knowledge without task-specific supervision.
3. Four commonsense benchmarks and six model sizes (up to 280B parameters) were analyzed.
4. Zero-shot performance improves with larger models, but not sufficiently to match human-level performance.
5. Few-shot evaluation (up to 64 examples) provides limited performance improvement.
6. Variations in prompt format and score functions significantly affect performance.
7. Answer-only baseline reveals that LMs often rely on surface cues rather than true commonsense reasoning.
8. Human-level performance requires models significantly larger than currently feasible (over 100 trillion parameters).
9. Alternative approaches like explicit commonsense supervision, multimodal grounding, or physical embodiment are suggested.
10. Properly reporting and comparing evaluation design choices is crucial for fair assessment.

# TAKEAWAYS:
1. Larger models alone are insufficient to achieve human-level commonsense understanding.
2. Few-shot learning offers limited gains, mainly for less natural text formats.
3. Evaluation design choices, such as prompt format and score function, greatly influence results.
4. Comparison against strong baselines, including answer-only, is essential for accurate assessment.
5. Future work should explore more efficient methods like multimodal learning for commonsense acquisition.
