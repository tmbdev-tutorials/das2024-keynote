# Pretrained Transformers as Universal Computation Engines

- Kevin Lu, UC Berkeley
- Pieter Abbeel, UC Berkeley
- Aditya Grover, Facebook AI Research
- Igor Mordatch, Google Brain

# Introduction
- Investigate transformer pretrained on natural language to generalize to other modalities
- Minimal finetuning: self-attention and feedforward layers frozen
- Study on sequence classification tasks: numerical computation, vision, protein fold prediction
- Compare performance with fully trained transformers and LSTMs

# Background/Related Work
- Transformers (Vaswani et al., 2017) success in NLP, vision, proteins, and multimodal tasks
- Classical RNN approaches (Rumelhart et al., 1985; Hochreiter & Schmidhuber, 1997)
- Transformers use self-attention for feature extraction across sequences
- Common practice: large models pre-trained on unsupervised tasks, finetuned on downstream tasks

# Contributions
- Introduce Frozen Pretrained Transformer (FPT)
- Show language-pretrained transformers improve performance on non-language tasks
- Analysis of architecture: comparing random initialized transformer to random LSTM

# Objective
- Investigate transformers' capability to generalize to different modalities
- Hypothesize self-attention layers can identify useful feature representations for any data sequence

# Methodology Overview
- Use GPT-2 pretrained on natural language data
- Finetune only linear input/output layers, positional embeddings, and layer norm parameters
- Evaluate on diverse classification tasks: numerical computation, vision, protein fold prediction

# Datasets
- Bit Memory: Predict masked bitstrings
- Bit XOR: Predict element-wise XOR of two bitstrings
- ListOps: Parse and evaluate list operations
- MNIST: Classify handwritten digits
- CIFAR-10: Classify images into 10 categories
- CIFAR-10 LRA: Modified CIFAR-10 with longer sequences
- Remote Homology: Predict protein fold from amino acid sequence

# Model Details
- Transformer architecture: frozen self-attention and feedforward layers
- Finetune linear input/output layers, positional embeddings, layer norm parameters
- ((Figure showing Frozen Pretrained Transformer architecture))

# Experiments
- Compare FPT with fully trained transformers and LSTMs
- Evaluate on diverse tasks: numerical computation, vision, protein fold prediction
- Analyze architecture: random initialization vs. language pretraining

# Results
- FPT achieves comparable performance to fully trained transformers
- Significant performance gains over LSTMs on long sequence tasks
- Faster convergence during training

# Performance Comparisons with Prior Work
- FPT vs. fully trained transformer and LSTM benchmarks
- Comparable or better performance on all tasks
- Highlights importance of language pretraining for transfer learning

# Ablation Studies
- Pretraining modality importance: language vs. random vs. bit memory vs. image
- Transformer vs. LSTM architecture: self-attention as effective inductive bias
- Finetuning specific parameters: layer norm, input/output layers

# Visualizations
- Attention weights visualization for bit tasks
- ((Figure showing attention weights for Bit XOR and Bit Memory tasks))

# Discussion
- Key insights: language-pretrained self-attention layers generalize well to different modalities
- Limitations: specific models and tasks analyzed, potential for different results with other models/tasks
- Potential biases in pretrained models

# Conclusion
- Language-pretrained transformers can generalize to non-language tasks with minimal finetuning
- Future work: investigate other data-rich modalities, multimodal pretraining, reinforcement learning applications

# References
- Vaswani et al., 2017
- Radford et al., 2019
- Tay et al., 2020
- Rao et al., 2019

# Acknowledgements
- Thanks to Luke Metz, Kimin Lee, Fangchen Liu, Roshan Rao, Aravind Srinivas, and others for feedback and discussions
