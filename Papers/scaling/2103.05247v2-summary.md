# ONE SENTENCE SUMMARY:
Pretrained language transformers, finetuned minimally, show strong performance on various non-language tasks, highlighting their potential as universal computation engines.

# MAIN POINTS:
1. Pretrained transformers can generalize to other modalities with minimal finetuning.
2. Frozen Pretrained Transformers (FPT) achieved strong performance across various sequence classification tasks.
3. FPTs matched or exceeded the performance of fully trained transformers and LSTMs on diverse tasks.
4. Pretraining on natural language improves performance and compute efficiency on non-language tasks.
5. Analysis shows language-pretrained transformers have advantageous properties for universal computation.
6. FPTs converge faster during training compared to random transformers.
7. The study compares the performance of FPT, fully trained transformers, and random LSTMs.
8. FPTs display strong results on numerical computation, vision, and protein fold prediction tasks.
9. Experimentation includes various model sizes and pretraining modalities.
10. The architecture of transformers contributes significantly to their performance on non-language tasks.

# TAKEAWAYS:
1. Minimal finetuning of pretrained transformers can achieve competitive performance on non-language tasks.
2. Pretraining on natural language data is beneficial for various downstream modalities.
3. FPTs provide a promising approach for efficient, general-purpose computation.
4. Increasing model size improves performance without significant overfitting.
5. Future work can explore hybrid pretraining across multiple data-rich modalities for even better results.
