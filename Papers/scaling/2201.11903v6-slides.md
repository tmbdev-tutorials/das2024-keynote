# Title Slide
- Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, Denny Zhou
- Google Research, Brain Team
- Presentation Date

# Introduction
- Problem: Enhancing reasoning in large language models
- Motivation: Improving performance on complex reasoning tasks
- Scope: Focus on arithmetic, commonsense, and symbolic reasoning

# Background/Related Work
- Prior Work: Natural language rationales and few-shot prompting
- Limitations: Costly rationale creation, poor performance on reasoning tasks
- Key Concepts: Chain-of-thought prompting

# Contributions
- Key Contributions: Introducing chain-of-thought prompting
- Novel Techniques: Providing intermediate reasoning steps as exemplars
- Impact: Significant performance improvements on reasoning tasks

# Objective
- Main Objectives: Elicit reasoning in large language models using chain-of-thought prompting
- Research Questions: Can chain-of-thought prompting enhance reasoning performance?

# Methodology Overview
- Approach: Few-shot prompting with intermediate reasoning steps
- Data Used: Arithmetic, commonsense, and symbolic reasoning benchmarks
- Model Architecture: Evaluation on multiple large language models

# Datasets
- Description: GSM8K, SVAMP, ASDiv, AQuA, MAWPS
- Data Sources: Publicly available datasets
- Preprocessing: Standard preprocessing and manual composition of few-shot exemplars

# Model Details
- Model Architecture: GPT-3, LaMDA, PaLM, UL2, Codex
- Key Components: Chain-of-thought sequences as exemplars
- Training Procedure: Prompting without finetuning

# Experiments
- Setup: Few-shot exemplars with chain-of-thought reasoning
- Evaluation Metrics: Accuracy on reasoning benchmarks

# Results
- Performance Metrics: Significant accuracy improvements
- Key Findings: Chain-of-thought prompting outperforms standard prompting

# Performance Comparisons with Prior Work
- Baselines: Standard prompting
- State-of-the-Art: Chain-of-thought prompting surpasses previous bests
- Performance Gains: Striking improvements, especially on challenging tasks

# Ablation Studies
- Variants: Equation only, variable compute only, reasoning after answer
- Findings: Chain-of-thought prompting most effective

# Visualizations
- Model Outputs: Examples of correct and incorrect chains of thought
- Accuracy Curves: Performance trends with different model scales
- Relevant Plots: Comparison of prompting methods

# Discussion
- Insights: Chain-of-thought prompting decomposes problems into steps
- Interpretation: Intermediate steps enhance model reasoning
- Limitations: Dependence on model scale, manual exemplar creation

# Conclusion
- Summary: Chain-of-thought prompting enhances reasoning in large language models
- Impact: Broad applicability to various reasoning tasks
- Future Work: Exploring smaller models, synthetic data generation

# References
- Cobbe et al., 2021
- Brown et al., 2020
- Kaplan et al., 2020
- Devlin et al., 2019
- Ling et al., 2017

# Acknowledgements
- Jacob Devlin, Claire Cui, Andrew Dai, Ellie Pavlick, Jacob Austin, Yuhuai Wu, Henryk Michalewski, Aitor Lewkowycz, Charles Sutton, Aakanksha Chowdhery

# Q&A
- Invitation for Questions
