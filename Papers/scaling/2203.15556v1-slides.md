# Training Compute-Optimal Large Language Models

- Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre
- DeepMind
- Date of Presentation

# Introduction

- Investigates optimal model size and token number for transformer language models under a compute budget
- Current large language models are significantly under-trained
- Proposes equal scaling of model size and training tokens for compute-optimal training
- Introduces Chinchilla, a model that outperforms larger models using the same compute budget

# Background/Related Work

- Recent large language models have over 500 billion parameters
- The cost for training large models is substantial
- Kaplan et al. (2020) showed a power law relationship between model size and performance
- Previous work suggested increasing model size more than training tokens
- This work suggests equal scaling of model size and training tokens

# Contributions

- Demonstrates that large models are under-trained
- Proposes equal scaling of model size and training tokens for compute-optimal training
- Introduces Chinchilla, which outperforms larger models
- Shows reduced inference cost and improved performance on downstream tasks

# Objective

- Determine the optimal trade-off between model size and number of training tokens under a fixed compute budget
- Test the hypothesis by training Chinchilla with 70 billion parameters on 1.4 trillion tokens

# Methodology Overview

- Empirical estimation of optimal model size and training tokens based on the losses of over 400 models
- Models range from 70 million to over 16 billion parameters and are trained on 5 to 500 billion tokens
- Three different approaches are used to estimate optimal scaling

# Datasets

- MassiveText dataset used for training
- Subsets of MassiveText: MassiveWeb, Books, C4, News, GitHub, Wikipedia
- Training data makeup adjusted for Chinchilla to account for increased number of training tokens

# Model Details

- Chinchilla has 70 billion parameters
- Trained on 1.4 trillion tokens
- Uses AdamW optimizer instead of Adam
- Modified SentencePiece tokenizer
- Trained on TPUv3/TPUv4 with JAX and Haiku

# Experiments

- Extensive evaluation of Chinchilla compared to Gopher and other large models
- Evaluated on language modeling, reading comprehension, question answering, common sense, MMLU, and BIG-bench tasks

# Results

- Chinchilla significantly outperforms Gopher and other large models on various tasks
- Achieves state-of-the-art accuracy on MMLU benchmark
- Outperforms Gopher on all subsets of The Pile
- Improved performance on reading comprehension and common sense benchmarks

# Performance Comparisons with Prior Work

- Chinchilla outperforms Gopher, GPT-3, Jurassic-1, and Megatron-Turing NLG on downstream tasks
- Uses less compute for fine-tuning and inference
- Substantial performance improvements with a smaller model trained on more data

# Discussion

- Key insights into the optimal scaling of model size and training tokens
- Interpretation of results shows benefits of a more optimally trained smaller model
- Limitations include the cost of training large models and the assumption of a power-law relationship

# Conclusion

- Summary of contributions: equal scaling of model size and training tokens, introduction of Chinchilla
- Impact of findings: improved performance with reduced compute for fine-tuning and inference
- Future work directions: further exploration of dataset scaling and understanding performance-toxicity trade-offs

# References

- Brown et al., 2020
- Kaplan et al., 2020
- Rae et al., 2021
- Thoppilan et al., 2022
- Lieber et al., 2021

# Acknowledgements

- Thanks to Jean-baptiste Alayrac, Kareem Ayoub, Chris Dyer, Nando de Freitas, Demis Hassabis, Geoffrey Irving, Koray Kavukcuoglu, Nate Kushman, Angeliki Lazaridou, Andy Brock, Irina Higgins, Michela Paganini, Francis Song, colleagues at DeepMind, and the JAX and XLA team.

# Q&A

- Invitation for Questions
