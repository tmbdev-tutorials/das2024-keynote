# Title Slide

- Emergent Abilities of Large Language Models
- Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus
- Google Research, Stanford University, UNC Chapel Hill, DeepMind
- Date of Presentation: 08/2022

# Introduction

- Problem Statement: Emergent abilities in large language models
- Motivation: Unpredictable phenomena not present in smaller models
- Scope: Exploration of emergent abilities with model scaling

# Background/Related Work

- Previous work on scaling laws (Devlin et al., 2019; Brown et al., 2020)
- Scaling curves for cross-entropy loss (Kaplan et al., 2020; Hoffmann et al., 2022)
- Emergence in other domains (Anderson, 1972; Hwang et al., 2012)

# Contributions

- Key Contributions: Survey of emergent abilities in large language models
- Novel Techniques: Few-shot prompting, augmented prompting strategies
- Impact: Raises questions about further scaling and new abilities

# Objective

- Main Objectives: Define emergent abilities, analyze scaling effects
- Research Questions: Why do emergent abilities occur? What enables them?

# Methodology Overview

- High-Level Approach: Analyze model performance across scales
- Data Used: Various benchmarks including BIG-Bench, TruthfulQA, MMLU
- Model Architecture: Pre-trained Transformer language models

# Datasets

- Description: BIG-Bench, TruthfulQA, MMLU
- Data Sources: Crowd-sourced benchmarks, curated datasets
- Data Preprocessing: Standard preprocessing steps for NLP tasks

# Model Details

- Model Architecture: Dense Transformer language models
- Key Layers: Transformer blocks, attention mechanisms
- Training Procedure: Pre-training with large-scale datasets

# Experiments

- Experimental Setup: Evaluate models on emergent tasks
- Evaluation Metrics: Accuracy, BLEU, exact match, cross-entropy loss

# Results

- Performance Metrics: Accuracy, BLEU, exact match
- Key Findings: Emergent abilities at specific scales, performance jumps

# Performance Comparisons with Prior Work

- Comparison with Baselines: Smaller models, prior versions
- Comparison with State-of-the-Art: GPT-3, LaMDA, PaLM
- Performance Gains: Substantial improvements at critical scales

# Visualizations

- Model Outputs: Few-shot prompting examples
- Accuracy Curves: Performance vs. model scale
- Other Plots: Cross-entropy loss, log probabilities ((Figure showing plots))

# Discussion

- Key Insights: Emergent abilities unpredictable, significant performance jumps
- Interpretation: Scaling enables new abilities, but exact reasons unclear
- Limitations: Dependence on high-quality data, optimal training not guaranteed

# Conclusion

- Summary: Emergent abilities observed at large scales
- Impact: Raises important questions for future research
- Future Work: Explore new architectures, training methods, and data scaling

# References

- Devlin et al., 2019
- Brown et al., 2020
- Kaplan et al., 2020
- Hoffmann et al., 2022
- Anderson, 1972

# Acknowledgements

- Charles Sutton, Slav Petrov, Douglas Eck, Jason Freidenfelds, Jascha Sohl-Dickstein, Ethan Dyer, Dale Schuurmans, Xavier Garcia

# Q&A

- Invitation for Questions
