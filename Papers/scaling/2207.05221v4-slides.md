# Language Models (Mostly) Know What They Know

- Authors: Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, Jared Kaplan
- Affiliation: Anthropic
- Date: November 21, 2022

# Introduction

- Investigates if language models can evaluate their own claims
- Larger models are well-calibrated on multiple choice and true/false questions
- Models can propose answers and then evaluate their correctness
- Performance improves when models consider many samples
- Trains models to predict "I know" (P(IK)) probabilities

# Background/Related Work

- Calibration of probabilistic predictions in language models
- Previous work on calibration for token probabilities
- Selective prediction and calibration in language models
- Recent studies on model calibration on diverse tasks

# Contributions

## Calibration

- Large models are well-calibrated on diverse multiple choice questions
- Calibration improves with model size and few-shot prompting
- True/False distinctions also show good calibration

## Self-Evaluation

- Models can self-evaluate their own samples as True or False
- Performance improves with model size
- Showing models many samples improves self-evaluation

## Training for P(IK)

- Models trained to predict if they know the answer (P(IK))
- P(IK) scores improve with model size
- P(IK) generalizes across tasks and with relevant source materials

# Objective

- Evaluate if language models can self-assess the validity of their outputs
- Investigate the calibration and generalization of these self-assessments
- Train models to predict "I know" probabilities

# Methodology Overview

- Analyze calibration on multiple choice and true/false tasks
- Self-evaluation by models on open-ended sampling tasks
- Training models to predict P(IK) without reference to specific answers

# Datasets

- Diverse multiple choice questions from BIG Bench and MMLU
- True/False tasks from various sources
- Generative tasks including TriviaQA, Lambada, Arithmetic, GSM8k, and Codex HumanEval

# Model Details

- Models with 800M, 3B, 12B, 52B parameters
- Pretrained on 850B tokens
- Evaluation on multiple choice tasks and generative tasks

# Experiments

- Calibration on multiple choice and true/false tasks
- Self-evaluation on generative tasks with T=True/False
- Training and evaluating P(IK) on diverse tasks
- Generalization of P(IK) to new tasks and source materials

# Results

- Larger models show better calibration and self-evaluation performance
- Calibration improves with model size and few-shot prompting
- P(IK) shows generalization across tasks and improves with relevant context

# Performance Comparisons with Prior Work

- Comparison with previous studies on calibration and selective prediction
- Improvements in calibration and self-evaluation with model size
- Enhanced performance with additional training on P(IK)

# Visualizations

- Calibration charts for multiple choice and true/false tasks
- Histograms of P(True) for self-evaluation
- P(IK) distributions and calibration plots

((Figure showing Calibration charts for various tasks))

# Discussion

- Large models can be well-calibrated and perform effective self-evaluation
- Verification improves faster than generation with model size
- Self-knowledge generalizes across tasks and with additional context
- Future work to address limitations and enhance honesty in AI models

# Conclusion

- Summary of key contributions and findings
- Impact of improved self-evaluation and P(IK) training
- Directions for future research on model honesty and generalization

# References

- [Guo et al., 2017] On calibration of modern neural networks
- [Desai and Durrett, 2020] Calibration of pre-trained transformers
- [Srivastava et al., 2022] Beyond the imitation game: Quantifying and extrapolating the capabilities of language models
- [Lin et al., 2022] Teaching models to express their uncertainty in words

# Acknowledgements

- Thanks to Paul Christiano, Owain Evans, Roger Grosse, Dan Hendrycks, Jacob Hilton, Geoffrey Irving, and Daniel Ziegler for comments
- Support from Daniela Amodei, Jarrah Bloomfield, Jamie Kerr, Timothy Telleen-Lawton, Jia Yuan Loke, Jeffrey Ladish, Rebecca Raible, Rune Kvist, Rob Gilson, Guro Khundadze, Filipe Dobreira, and Sebastian Conybeare

# Q&A

- Invitation for questions
