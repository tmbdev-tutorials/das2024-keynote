# ONE SENTENCE SUMMARY:

The study demonstrates that large language models can self-evaluate their accuracy and predict their own knowledge with reasonable calibration and generalization across tasks.

# MAIN POINTS:

1. Larger models are well-calibrated on multiple choice and true/false questions.
2. Self-evaluation involves models proposing answers and then assessing their correctness probability.
3. Model size and few-shot prompting improve calibration on multiple choice questions.
4. Self-evaluation accuracy increases when models consider many of their own samples.
5. Models can predict their own knowledge probability (P(IK)) with decent accuracy.
6. P(IK) generalizes across tasks but struggles with calibration on new tasks.
7. Relevant source materials and hints improve P(IK) predictions.
8. Models trained on diverse datasets show better generalization for P(IK).
9. Calibration improves with model size and few-shot examples.
10. Techniques like temperature adjustment can remediate RLHF policy miscalibration.

# TAKEAWAYS:

1. Larger language models can effectively self-evaluate their answers.
2. Calibration and few-shot prompting are crucial for accurate self-assessment.
3. Models show promising generalization in predicting their knowledge across various tasks.
4. Relevant context and hints significantly enhance model self-evaluation.
5. Continuous improvement in model size and training diversity boosts calibration and generalization.
