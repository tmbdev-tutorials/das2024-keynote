# Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text

- Zhun Yang, Adam Ishay
- Arizona State University
- Joohyung Lee
- Arizona State University, Samsung Research
- Date of Presentation

# Introduction

- Problem: Large language models (LLMs) lack robust reasoning abilities.
- Motivation: Combine LLMs with logic programming for better reasoning.
- Scope: Demonstrate improved performance in multiple NLP benchmarks.

# Background/Related Work

- Previous Work: Neural networks for QA tasks often lack true reasoning.
- LLMs: Show general reasoning but struggle with specific benchmarks.
- Logic Programming: Provides interpretable and explainable reasoning.

# Contributions

- Key Contributions: Combining LLMs with logic programming.
- Novel Techniques: Use LLMs as few-shot semantic parsers for logic programs.
- Impact: Achieves state-of-the-art performance in multiple benchmarks.

# Objective

- Main Objective: Enhance reasoning capabilities of LLMs using logic programming.
- Research Questions: Can LLMs effectively parse natural language into logical forms? Can this combination improve QA performance without retraining?

# Methodology Overview

- Approach: Use LLMs to parse natural language into logical forms for ASP.
- Data: Various NLP benchmarks and robot planning tasks.
- Model: LLMs (e.g., GPT-3) combined with Answer Set Programming (ASP).

# Datasets

- Description: Benchmarks like bAbI, StepGame, CLUTRR, and gSCAN.
- Data Sources: Publicly available datasets.
- Preprocessing: Semantic parsing using LLMs to convert text into logical forms.

# Model Details

- Model Architecture: LLMs for parsing, ASP for reasoning.
- Key Components: Semantic parsing prompts, ASP knowledge modules.
- Training: No retraining required, few-shot examples guide LLM adaptation.

# Experiments

- Setup: Evaluate on bAbI, StepGame, CLUTRR, and gSCAN datasets.
- Metrics: Accuracy in QA tasks, ability to identify dataset errors.

# Results

- Performance: Achieves state-of-the-art accuracy in all tested benchmarks.
- Key Findings: LLMs are effective few-shot parsers, ASP provides robust reasoning.

# Performance Comparisons with Prior Work

- Comparison with Baselines: Outperforms LLM-only models.
- Comparison with State-of-the-Art: Matches or exceeds performance of specialized models.
- Gains: Significant improvement in handling diverse QA tasks.

# Visualizations

- Accuracy Curves: Performance across different benchmarks.
- Model Outputs: Examples of parsed logical forms and ASP results.
- Error Analysis: Identified dataset errors using the combined approach.

# Discussion

- Insights: LLMs excel at parsing, ASP excels at reasoning.
- Interpretation: Combination offers robust, interpretable reasoning.
- Limitations: Dependency on manually crafted knowledge modules.

# Conclusion

- Summary: Effective combination of LLMs and ASP for robust reasoning.
- Impact: Improved QA performance without extensive retraining.
- Future Work: Automate knowledge module creation, expand to more complex tasks.

# References

- Brewka et al., 2011. Answer Set Programming at a Glance.
- Lifschitz, 2008. What is Answer Set Programming?
- Ruder, 2021. Challenges and Opportunities in NLP Benchmarking.
- Valmeekam et al., 2022. Large Language Models Still Canâ€™t Plan.
- Weston et al., 2016. Towards AI-Complete Question Answering.

# Acknowledgements

- Supported by the National Science Foundation under Grant IIS-2006747.

# Q&A

- Invitation for Questions
