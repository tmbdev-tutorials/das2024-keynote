# Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata

- Bohui Zhang, Ioannis Reklos, Nitisha Jain, Albert Meroño Peñuela, Elena Simperl
- Department of Informatics, King’s College London, London, UK
- Date of Presentation

# Introduction

- Use of Large Language Models (LLMs) for knowledge engineering tasks
- Focus on ISWC 2023 LM-KBC Challenge
- Task: Given subject and relation pairs from Wikidata, predict relevant objects and link them to Wikidata QIDs
- Developed a pipeline using LLMs for Knowledge Engineering (LLMKE)
- Achieved a macro-averaged F1-score of 0.701

# Background/Related Work

- Language models successful in various NLP tasks: text classification, sentiment analysis, named entity recognition
- Recent advancements in LLMs: ChatGPT, GPT-4, LLaMa, Claude2, Bard
- LLMs viewed as knowledge repositories, leading to focus on prompt engineering
- Knowledge graphs (KGs) for knowledge representation and reasoning
- Challenges in using LLMs for KG completion and correction

# Contributions

- Developed LLMKE pipeline for knowledge engineering using LLMs
- Combined knowledge probing and Wikidata entity mapping
- Demonstrated significant variability in LLM knowledge across domains
- Won Track 2 of the ISWC 2023 LM-KBC Challenge

# Objective

- Main objective: Use LLMs to predict object entities given subject and relation pairs from Wikidata
- Investigate the effectiveness of LLMs in improving the efficiency of knowledge engineering
- Evaluate the potential of LLMs for automatic Knowledge Base completion and correction

# Methodology Overview

- Two main steps: knowledge probing and Wikidata entity mapping
- Used pre-trained LLMs: gpt-3.5-turbo and GPT-4
- Employed in-context learning approaches
- Achieved macro-average F1 score of 0.701

# Datasets

- Dataset from ISWC 2023 LM-KBC Challenge
- Queried from Wikidata, covering 21 relation types across 7 domains
- 1,940 statements for each train, validation, and test sets
- Minimum and maximum number of object-entities per relation varies from 0 to 20

# Model Details

- Knowledge probing: engineered prompt templates for LLMs
- Three settings: question prompting, triple completion prompting, retrieval-augmented context
- Wikidata entity mapping: used MediaWiki Action API for entity search
- Improved disambiguation methods: case-based, keyword-based, LM-based

# Experiments

- Experimental setup included few-shot learning with examples from the training set
- Retrieval-augmented context used Wikipedia and domain-specific websites
- Evaluated model performance using precision, recall, and F1-score

# Results

- GPT-4 outperformed gpt-3.5-turbo
- Retrieval-augmented context setting showed best performance
- Significant variability in performance across different relations
- Relations with limited domain/range performed better

# Performance Comparisons with Prior Work

- LLMKE achieved state-of-the-art results in the ISWC 2023 LM-KBC Challenge
- Comparison with baselines and state-of-the-art models showed performance gains
- Highlighted the impact of improved disambiguation methods

# Discussion

- Key insights on variability of LLM knowledge across domains
- Limitations in fully automatic knowledge engineering
- Importance of human-in-the-loop for accuracy
- Potential of LLMs in collaborative knowledge engineering and improving Wikidata quality

# Conclusion

- Developed a pipeline using LLMs for knowledge engineering tasks
- Achieved significant results in the ISWC 2023 LM-KBC Challenge
- Highlighted the need for further experimentation and human oversight
- Future work: Enhance LLM capabilities for logical reasoning and domain-specific knowledge

# References

- OpenAI, GPT-4 Technical Report, 2023
- H. Touvron et al., LLaMA: Open and Efficient Foundation Language Models, 2023
- T. Shin et al., AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts, 2020
- G. Qin, J. Eisner, Learning How to Ask: Querying LMs with Mixtures of Soft Prompts, 2021
- L. Ehrlinger, W. Wöß, Towards a Definition of Knowledge Graphs, 2016

# Acknowledgements

- Funded by the HE project MuseIT, co-founded by the European Union under the Grant Agreement No 101061441
- Views and opinions expressed are those of the authors and do not necessarily reflect those of the European Union or European Research Executive Agency

# Q&A

- Invitation for Questions
