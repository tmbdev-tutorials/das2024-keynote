# Factuality of Large Language Models in the Year 2024
- Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Georgiev, Rocktim Jyoti Das, Preslav Nakov
- MBZUAI, Monash University, LibrAI, Google, Sofia University
- Date of Presentation: 2024

# Introduction
- Large language models (LLMs) have become integral in daily life.
- LLMs provide straightforward answers, reducing the need for manual information search.
- However, LLM responses are often factually incorrect.
- Assessing and improving LLM factuality is a critical research area.

# Background
- LLMs tend to fabricate ungrounded statements, leading to misinformation.
- Researchers have developed datasets and measures to evaluate LLM factuality.
- Various strategies leverage external knowledge to mitigate factual errors.
- Recent surveys on LLM hallucinations and factuality are lengthy and complex.

# Contributions
- Focus on recent, novel works in LLM factuality.
- Summarize common bottlenecks and solutions.
- Provide insights based on practical experience.
- Discuss ambiguous concepts and evaluation approaches.

# Objective
- Identify major challenges and potential solutions for improving LLM factuality.
- Analyze obstacles to automated factuality evaluation.
- Offer outlook on future research directions.

# Methodology Overview
- High-level overview of approaches to improve factuality.
- Discuss pre-training, fine-tuning, and reinforcement learning with feedback.
- Emphasize retrieval augmentation and post-processing strategies.

# Datasets
- Categorize datasets into four types: open-ended generation, Yes/No answers, short-form answers, and multiple-choice QA.
- Highlight the challenges in automatic evaluation for long-form open-ended generations.

# Model Details
- Model architecture involves pre-training on vast datasets.
- Fine-tuning on domain-specific data to enhance factuality.
- Use of retrieval augmentation to anchor models in external knowledge.

# Experiments
- Experimental setup includes benchmarks for factual accuracy.
- Evaluation metrics involve accuracy, F1 scores, and human annotations.
- Comparison of different approaches to assess effectiveness.

# Results
- Performance metrics show improvements in factual accuracy.
- Key findings indicate that retrieval augmentation and fine-tuning are effective.
- Challenges remain in achieving consistent factuality across different contexts.

# Performance Comparisons with Prior Work
- Comparison with baselines and state-of-the-art models.
- Performance gains observed with retrieval-augmented models.
- Trade-offs between factual accuracy and response creativity.

# Visualizations
- Accuracy curves comparing different models.
- Performance metrics across various datasets.
- ((Figure showing performance comparison across different approaches))

# Discussion
- Key insights on improving LLM factuality.
- Interpretation of results and limitations of current methods.
- Challenges in automated evaluation and latency issues in retrieval systems.

# Conclusion
- Summary of contributions and impact on the field.
- Identification of major challenges and future research directions.
- Emphasis on the need for unified evaluation frameworks and efficient fact-checkers.

# References
- Tonmoy et al., 2024: Comprehensive survey of hallucination mitigation techniques.
- Chen et al., 2023: FELM: Benchmarking factuality evaluation of large language models.
- Gao et al., 2023b: Retrieval-augmented generation for large language models.
- Huang et al., 2023: A survey on hallucination in large language models.
- Rawte et al., 2023b: Survey of hallucination in large foundation models.

# Acknowledgements
- Acknowledge contributions from collaborators and funding agencies.

# Q&A
- Invitation for questions and discussion.
