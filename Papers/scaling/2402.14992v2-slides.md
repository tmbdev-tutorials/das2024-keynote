# tinyBenchmarks: Evaluating LLMs with Fewer Examples
- Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, Mikhail Yurochkin
- University of Michigan, University of Pompeu Fabra, IBM Research, MIT-IBM Watson AI Lab
- 41st International Conference on Machine Learning, Vienna, Austria, 2024

# Introduction
- Large Language Models (LLMs) exhibit diverse capabilities across tasks
- Benchmarks like MMLU and Open LLM Leaderboard test these models extensively
- Full benchmark evaluations are computationally and financially expensive
- This paper explores methods to reduce the number of examples needed for accurate performance evaluation

# Background/Related Work
- Traditional benchmarks involve tens of thousands of examples
- Evaluating LLMs on these benchmarks is costly (e.g., HELM costs over 4K GPU hours)
- Previous work includes stratified sampling and clustering methods to reduce evaluation costs
- Item Response Theory (IRT) from psychometrics is introduced to model LLM evaluation

# Contributions
- Introduce methods to reduce the number of evaluation examples for LLMs
- Show that 100 curated examples can estimate performance within 2% of the true accuracy
- Release tools and tiny versions of popular benchmarks: Open LLM Leaderboard, MMLU, HELM, AlpacaEval 2.0
- Empirical analysis demonstrating the reliability and efficiency of the proposed methods

# Objective
- Assess the performance of LLMs using fewer examples
- Investigate strategies like stratified sampling, clustering, and IRT-based methods
- Evaluate these strategies on popular benchmarks to confirm their effectiveness

# Methodology Overview
- Evaluate LLMs using a fraction of the original benchmark examples
- Methods: Stratified random sampling, clustering based on correctness, IRT-based clustering
- Four benchmarks: Open LLM Leaderboard, MMLU, HELM, AlpacaEval 2.0
- Performance estimation using IRT models to improve accuracy

# Datasets
- Open LLM Leaderboard: 6 scenarios, approx. 29K examples, 395 LLMs
- MMLU: 57 subjects, approx. 14K examples, subset of Open LLM Leaderboard
- HELM Lite v1.0.0: 10 core scenarios, approx. 10K examples, 30 models
- AlpacaEval 2.0: 100 LLMs, 805 examples, requires GPT-4 as a judge

# Model Details
- Stratified random sampling: Ensures representation of different data groups
- Clustering: Groups examples by model correctness, selects representative examples (anchor points)
- IRT-based clustering: Uses IRT model to create meaningful representations of examples, clusters these representations

# Experiments
- Train-test split: Random split and by date (recent models for testing)
- Evaluation strategies: Vanilla methods and IRT-enhanced methods (random++, correctness++, IRT++)
- Metrics: Estimation error, rank correlation

# Results
- 100 examples per scenario sufficient for performance estimation within 2% error
- IRT-based methods consistently perform well across benchmarks
- Robustness to distribution shifts: Effective even for newer, more capable LLMs
- Specialized LLMs: IRT-based methods are more robust compared to correctness-based clustering

# Visualizations
- ((Figure showing performance estimation error per benchmark))
- ((Figure showing predicted vs. true performance for recent LLMs))
- ((Figure showing estimation error for specialized LLMs))

# Conclusion
- Demonstrated efficient LLM evaluation with significantly fewer examples
- Released tinyBenchmarks and IRT-based tools for efficient evaluation
- Future work: Adaptive testing, prompt evaluation

# References
- Brown et al., 2020; Beeching et al., 2023; Liang et al., 2022; Li et al., 2023
- Additional key references in NLP and psychometrics

# Acknowledgements
- National Science Foundation (NSF) grants no. 2027737 and 2113373
- Yotam Perlitz for assistance with HELM data

# Q&A
- Invitation for questions and discussion on the presented work
