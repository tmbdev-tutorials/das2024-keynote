# Summary

## ONE SENTENCE SUMMARY:
The paper introduces "tinyBenchmarks," a method to evaluate large language models (LLMs) using significantly fewer examples, maintaining accuracy and reducing computational costs.

## MAIN POINTS:
1. LLM evaluation is costly due to large benchmark datasets.
2. tinyBenchmarks reduces the number of evaluation examples needed.
3. 100 examples are sufficient for accurate LLM performance estimation on MMLU.
4. Released tools include tiny datasets and evaluation strategies.
5. Evaluation strategies include stratified random sampling and clustering.
6. Item Response Theory (IRT) models enhance performance estimation.
7. Experiments show tinyBenchmarks can reproduce original evaluation results with 2% error.
8. tinyBenchmarks effectively evaluates LLMs with fewer examples, saving costs.
9. IRT-based methods provide robust performance across various benchmarks.
10. tinyBenchmarks and tools are publicly available for efficient LLM evaluation.

## TAKEAWAYS:
1. tinyBenchmarks significantly reduce the computational, environmental, and financial costs of LLM evaluation.
2. Evaluating LLMs on 100 curated examples can accurately estimate performance.
3. IRT-based methods consistently provide reliable performance estimation.
4. Tools and tiny datasets are available for easy implementation.
5. The approach is robust even with newer, more capable LLMs.
