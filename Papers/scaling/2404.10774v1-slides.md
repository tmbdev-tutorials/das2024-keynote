# Title Slide

- MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents
- Liyan Tang, Philippe Laban, Greg Durrett
- The University of Texas at Austin, Salesforce AI Research
- Date of Presentation

# Introduction

- Recognizing if LLM output can be grounded in evidence is crucial in NLP tasks.
- Current fact-checking approaches are computationally expensive.
- Aim to build small models with GPT-4 level performance but 400x cheaper.
- Introduce synthetic training data and a new benchmark, LLM-AGGREFACT.

# Background/Related Work

- Hallucination problem in LLMs: generating unsupported information.
- Previous work focuses on either closed-book or grounded generation settings.
- Need for accurate and efficient fact-checking implementations.

# Contributions

- Develop synthetic data generation methods for training fact-checkers.
- Introduce LLM-AGGREFACT benchmark for unified evaluation.
- MiniCheck-FT5 outperforms comparable models and approaches GPT-4 accuracy.

# Objective

- Main objective: build efficient fact-checking models for grounding documents.
- Hypothesis: Synthetic data can train small models to achieve high fact-checking accuracy.

# Methodology Overview

- Use GPT-4 to create synthetic training data with realistic factual errors.
- Train small models to check each fact in claims and recognize synthesis across sentences.
- Evaluate using the unified benchmark LLM-AGGREFACT.

# Datasets

- LLM-AGGREFACT: aggregates 10 existing datasets for fact-checking and grounding.
- Datasets include claims with human-annotated factual errors.
- Covers diverse domains: news, dialogue, science, healthcare.

# Model Details

- MiniCheck is based on Flan-T5 fine-tuned on synthetic and entailment data.
- Use DeBERTa, RoBERTa, and Flan-T5 as backbone models.
- Training involves cross-entropy loss and specific hyperparameters.

# Experiments

- Evaluate models on LLM-AGGREFACT without and with threshold tuning.
- Compare specialized fact-checkers and LLM-based fact-checkers.
- Analyze performance with claim decomposition and decontextualization.

# Results

- MiniCheck-FT5 achieves a 4.3% improvement over AlignScore.
- Performance on par with top LLM-based fact-checkers.
- Significant cost reduction: MiniCheck-FT5 is 400x cheaper than GPT-4.

# Performance Comparisons with Prior Work

- MiniCheck models outperform specialized evaluators like AlignScore.
- Achieve similar performance to non-frontier LLM-based fact-checkers.
- Robustness demonstrated through minimal performance drop without threshold tuning.

# Discussion

- Synthetic data effectively trains models for multi-fact and multi-sentence reasoning.
- Claim decomposition doesn't consistently improve performance.
- Decontextualization shows limited impact on current benchmark.

# Conclusion

- Introduced efficient fact-checking methods with synthetic data.
- Developed LLM-AGGREFACT for comprehensive evaluation.
- MiniCheck models offer high accuracy at a fraction of the cost.

# References

- Falke et al., 2019; Maynez et al., 2020; McKenna et al., 2023
- Tang et al., 2024; Gao et al., 2023; Malaviya et al., 2024
- Kryscinski et al., 2020; Fabbri et al., 2022; Laban et al., 2023

# Acknowledgements

- Thanks to Jessy Li for comments on the draft.
- Supported by Amazon, NSF CAREER Award IIS-2145280, NSF AI Institute for Foundations of Machine Learning.
- Additional support from Open Philanthropy, UT Austin Office of the Vice President for Research, and Good Systems.

# Q&A

- Invitation for Questions
