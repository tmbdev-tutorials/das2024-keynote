# MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark

- Authors: Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen
- Affiliations: University of Waterloo, University of Toronto, Carnegie Mellon University
- Date of Presentation: June 23, 2024

# Introduction

- Advances in large language models (LLMs) have transformed natural language processing (NLP)
- Benchmarks like MMLU have been pivotal for evaluating LLMs
- MMLU performance has plateaued with new models, making it hard to discern differences
- Introduction of MMLU-Pro to address these issues

# Background/Related Work

- Existing benchmarks: AGIEval, ARC, BBH, and MMLU
- MMLU includes 57 subjects across various domains
- Saturation observed in MMLU with top models achieving similar scores
- Sensitivity of MMLU to prompt variations

# Contributions

- MMLU-Pro introduces more challenging, reasoning-focused questions
- Expands choice set from four to ten options
- Eliminates trivial and noisy questions in MMLU
- Shows greater stability under varying prompts

# Objective

- Enhance the MMLU benchmark to better track progress in LLMs
- Integrate more complex reasoning questions
- Improve robustness against prompt variations

# Methodology Overview

- High-Level Overview: Enhanced dataset with reasoning-focused questions
- Data Sources: Original MMLU, STEM Website, TheoremQA, SciBench
- Model Architecture: Evaluation of models using Chain of Thought (CoT) reasoning and direct answering

# Datasets

- 14 diverse domains including mathematics, physics, chemistry, law, engineering, psychology, and health
- Over 12,000 questions sourced and filtered
- Data Sources: Original MMLU, STEM Website, TheoremQA, SciBench

# Model Details

- Evaluation of more than 50 LLMs including GPT-4o, Claude-3-Opus, and Gemini
- Use of CoT reasoning to achieve better performance
- Comparison between CoT and direct answering methods

# Experiments

- Experimental Setup: 5-shot CoT approach
- Evaluation Metrics: Accuracy, robustness to prompt variations

# Results

- Performance Metrics: Significant drop in accuracy from MMLU to MMLU-Pro
- Key Findings: MMLU-Pro is more discriminative and challenging
- Leading model GPT-4o achieves 72.6% accuracy on MMLU-Pro

# Performance Comparisons with Prior Work

- Comparison with Baselines: GPT-4o, GPT-4-Turbo, Claude-3-Opus
- Performance Gains: CoT reasoning shows significant improvement on MMLU-Pro
- Performance Trade-offs: Lower accuracy but higher discriminative power

# Visualizations

- Performance comparison between MMLU and MMLU-Pro
- Accuracy distributions affected by 24 prompts
- Performance using CoT vs. Direct answering

    ((Figure showing performance comparison, accuracy distributions, and CoT vs. Direct answering))

# Discussion

- Key Insights: MMLU-Pro presents more complex reasoning challenges
- Interpretation of Results: CoT significantly improves performance on MMLU-Pro
- Limitations: Still constrained by multiple-choice format, excludes multi-modal models

# Conclusion

- Summary of Contributions: MMLU-Pro offers a more robust and challenging benchmark
- Impact of Findings: Better tracks progress in LLM capabilities
- Future Work Directions: Explore open-ended questions, multi-modal assessments

# References

- Hendrycks et al., Measuring Massive Multitask Language Understanding, 2020
- Wei et al., Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, 2022
- Liang et al., Holistic Evaluation of Language Models, 2022
- Sanh et al., Multitask Prompted Training Enables Zero-Shot Task Generalization, 2021
- Srivastava et al., Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 2022

# Acknowledgements

- Thanks to Reddit user Dorrin Verrakai, Ankesh Anand from Google DeepMind, and Ning Shang from Microsoft for their feedback
- Contributions of open-source language model providers

# Q&A

- Invitation for Questions
