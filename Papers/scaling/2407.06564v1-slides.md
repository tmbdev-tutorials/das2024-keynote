# Combining Knowledge Graphs and Large Language Models
- Amanda Kau, Xuzeng He, Aishwarya Nambissan
- Aland Astudillo, Hui Yin, Amir Aryani
- Australian National University, Swinburne University of Technology
- Date of Presentation: 2024

# Introduction
- NLP advancements attributed to large datasets and computing power
- LLMs developed (e.g., BERT, T5, GPT series)
- Applications: language translation, content creation, virtual assistants
- LLMs' limitations: hallucinations, lack of domain-specific knowledge
- KGs mitigate LLMs' limitations by structuring knowledge

# Background/Related Work
- Early surveys on KEPLMs (Wei et al. 2021, Zhen et al. 2022, Hu et al. 2023)
- KGs as structured knowledge sources for LLMs
- Challenges in KG construction: entity extraction, knowledge fusion, coreference resolution
- Use of LLMs to enhance KG construction

# Contributions
- Systematic analysis of 28 papers on KG-powered LLMs, LLM-based KGs, hybrid approaches
- Comprehensive overview of trends, techniques, and challenges
- Benefits for new researchers and deepening understanding of KG and LLM integration

# Objective
- Research Questions:
  - How can KGs enhance LLM capabilities?
  - How can LLMs support and enhance KGs?
  - Are there advantages in combining KGs and LLMs more jointly?

# Methodology Overview
- Search on arXiv from Feb 2024 to May 2024
- Keywords: "Large Language Model", "Knowledge Graph"
- Criteria: original research on LLMs and KGs, either KG-empowered LLMs, LLM-empowered KGs, or hybrid approaches

# Datasets
- Reviewed papers from arXiv related to LLMs and KGs
- Focused on articles published within the last five years
- Selected based on relevance to the research questions

# Model Details
## LLMs
- Based on transformers architecture with self-attention mechanism
- Tokenization process for input prompts
- Generation of tokens until end or length limit
## KGs
- Directed labelled graphs representing entities and relationships
- Data presented as (subject, object, predicate) triples
- Construction involves knowledge acquisition, refinement, and evolution

# Experiments
- Analysis of methods for KG-powered LLMs and LLM-based KGs
- Evaluation of hybrid approaches combining KGs and LLMs
- Thematic analysis categorizing models into "Add-ons" vs. "Joint" approaches

# Results
- Performance improvement by combining KGs and LLMs
- Enhanced semantic understanding and model interpretability
- Hybrid models show better results in knowledge-driven tasks

# Performance Comparisons with Prior Work
- Comparison with vanilla pre-trained models
- Evaluation of hybrid models vs. standalone KG or LLM approaches
- Notable improvements in specific domains like entity typing and question answering

# Thematic Analysis
## Add-ons
- Models using KGs and LLMs as supplementary tools
- Examples: KnowPhish, BEAR
## Joint
- Models leveraging combined strengths of KGs and LLMs
- Examples: K-BERT, ERNIE, LMExplainer

# Discussion
- Insights on the effectiveness of KG and LLM integration
- Limitations: domain-specific KGs, computational resources, outdated knowledge
- Potential for future research in multimodal LLMs and smaller integrated models

# Conclusion
- Summary of findings on KG and LLM integration
- Joint approach offers significant performance improvements
- Future research directions: effective knowledge integration, multimodal KG-LLM combinations

# References
- Devlin et al., 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- Radford et al., 2018. Improving Language Understanding by Generative Pre-Training
- Brown et al., 2020. Language Models are Few-Shot Learners
- Wei et al., 2021. Knowledge Enhanced Pretrained Language Models: A Comprehensive Survey
- Yang et al., 2024. Enhancing Large Language Models with Knowledge Graphs for Fact-Aware Language Modeling

# Acknowledgements
- Funding sources
- Collaborators and institutions

# Q&A
- Invitation for questions and discussion
