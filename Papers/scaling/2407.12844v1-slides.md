# Title Slide

- Metabench: A Sparse Benchmark to Measure General Ability in Large Language Models
- Alex Kipnis, Konstantinos Voudouris, Luca M. Schulze Buschoff, Eric Schulz
- Human-Centered AI, Helmholtz Munich; University of Cambridge
- Date of Presentation: [Insert Date]

# Introduction

- Problem: Evaluating diverse abilities of Large Language Models (LLMs) is challenging.
- Motivation: Existing benchmarks are extensive and often redundant.
- Scope: Introduces metabench, a compressed benchmark derived from six prominent benchmarks.

# Background/Related Work

- Traditional benchmarks include ARC, GSM8K, HellaSwag, MMLU, TruthfulQA, and WinoGrande.
- Redundancy in benchmarks leads to inefficiency.
- Prior work on tinyBenchmarks demonstrates potential for reduced benchmark sizes.

# Contributions

- Developed metabench, a sparse benchmark distilling six major benchmarks.
- Metabench retains less than 3% of the original size while maintaining evaluative power.
- Provides estimates of underlying abilities, not just point scores.

# Objective

- Main Objective: Create a more efficient and informative benchmark for LLMs.
- Research Questions: Can a smaller set of items maintain accurate evaluation? What are the underlying abilities measured?

# Methodology Overview

- Analyzed data from over 5000 LLMs.
- Identified most informative items from six benchmarks.
- Created a sparse benchmark and tested its efficacy.

# Datasets

- Benchmarks used: ARC, GSM8K, HellaSwag, MMLU, TruthfulQA, WinoGrande.
- Total items: 28,632.
- Data Source: Open LLM Leaderboard.

# Model Details

## Preprocessing Steps

- Removed items based on variance, difficulty, and correlation with scores.
- Cross-validated subsampling to 350 items per benchmark.
- Used IRT models to estimate item parameters and select the most informative items.

## Item Response Theory (IRT)

- IRT models relationship between ability and probability of correct response.
- Used 2PL, 3PL, and 4PL models to fit item data.
- Estimated abilities using Maximum A Posteriori (MAP) and Expected A Posteriori (EAPsum).

# Experiments

- Experimental Setup: Cross-validated subsampling and score reconstruction.
- Evaluation Metrics: Root Mean Square Error (RMSE) and Spearman correlations.

# Results

## Score Reconstruction

- Reconstructed original benchmark scores with an average RMSE of 1.5%.
- Reconstructed total score with an RMSE of 0.8%.
- High Spearman correlation (r = 0.93) between estimated abilities and total scores.

## Performance Comparisons with Prior Work

- Metabench outperforms or matches random subsampling in RMSE.
- Significant reduction in benchmark size with minimal loss of information.

# Visualizations

- ((Figure showing processing pipeline))
- ((Figure showing score reconstruction and RMSE distributions))

# Discussion

- Key Insights: Metabench efficiently measures LLM abilities with fewer items.
- Limitations: Potential bias due to non-independent LLM responses.
- Interpretation: High correlation among benchmarks suggests a single underlying factor.

# Conclusion

- Summary: Metabench is an efficient, sparse benchmark for LLMs.
- Impact: Reduces evaluation redundancy and computational cost.
- Future Work: Validate across more LLMs and extend to other domains.

# References

- Brown et al., 2020. "Language models are few-shot learners."
- Bubeck et al., 2023. "Sparks of artificial general intelligence."
- Clark et al., 2018. "Think you have solved question answering? Try ARC."
- Cobbe et al., 2021. "Training verifiers to solve math word problems."
- Lin et al., 2022. "TruthfulQA: Measuring how models mimic human falsehoods."

# Acknowledgements

- Acknowledgement to contributors and institutions.

# Q&A

- Invitation for Questions
