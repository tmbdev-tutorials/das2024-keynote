## ONE SENTENCE SUMMARY:

Metabench, a sparse benchmark derived from six prominent benchmarks, efficiently measures large language models' abilities while maintaining high fidelity and reducing redundancy.

## MAIN POINTS:

1. Large Language Models (LLMs) vary in abilities across multiple tasks.
2. Traditional benchmarks often measure overlapping skills, leading to inefficiency.
3. Metabench distills six major benchmarks into less than 3% of their original size.
4. Six benchmarks used: ARC, GSM8K, HellaSwag, MMLU, TruthfulQA, and WinoGrande.
5. Metabench can reconstruct original benchmark scores with minimal error.
6. The framework uses psychometric techniques to estimate underlying abilities.
7. Metabench shows high correlation with original scores (Spearman correlation r = 0.93).
8. Adaptive testing can further reduce the number of items administered.
9. Metabench offers a streamlined, cost-effective way to evaluate LLMs.
10. All analyses and results are available on GitHub.

## TAKEAWAYS:

1. Metabench efficiently measures LLMs' abilities using a significantly smaller set of items.
2. It maintains high fidelity to original benchmarks with minimal information loss.
3. The benchmark's psychometric approach provides deeper insights into LLM competencies.
4. Adaptive testing within metabench can further enhance evaluation efficiency.
5. Metabench's findings support the existence of a general underlying ability across benchmarks.
