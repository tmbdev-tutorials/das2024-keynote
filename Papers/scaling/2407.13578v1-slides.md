# Large Language Models as Reliable Knowledge Bases?

- Danna Zheng, Mirella Lapata, Jeff Z. Pan
- School of Informatics, University of Edinburgh, UK
- Huawei Edinburgh Research Centre, CSI, UK
- Date of Presentation: [Insert Date]

# Introduction

- Problem: Can Large Language Models (LLMs) serve as reliable knowledge bases (KBs)?
- Motivation: LLMs have shown potential in encoding knowledge, but their reliability as KBs is under-explored.
- Scope: Evaluating the factuality and consistency of LLMs as KBs for seen and unseen knowledge.

# Background/Related Work

- Previous studies have explored the relationship between LLMs and KBs.
- Common methods: Converting knowledge graphs into questions and assessing LLMs' ability to answer.
- Limitation: Memorizing facts is not enough for LLMs to be reliable KBs.

# Contributions

- Define criteria for reliable LLM-as-KB focusing on factuality and consistency.
- Propose metrics to evaluate these criteria.
- Comprehensive analysis of 26 LLMs considering model size, instruction tuning, and in-context learning (ICL).

# Objective

- Main Objectives: Assess the reliability of LLMs as KBs.
- Hypotheses: LLMs can provide factual and consistent responses for both seen and unseen knowledge.

# Methodology Overview

- Criteria: Factuality (correct and uninformative rates) and consistency (consistent responses).
- Metrics: Net Correct Rate (NCR), Uninformative Rate (UR), Consistency Scores (Ccorrect, Cwrong).
- Evaluation: 26 LLMs on datasets for seen and unseen knowledge.

# Datasets

## SeenQA Dataset

- Sources: Natural Questions, TriviaQA, PopQA.
- Selection: 3,000 questions filtered for factoid nature and time-sensitivity.

## UnseenQA Dataset

- Creation: 3,000 questions from templates covering different answer types (number, person, time, location, others).
- Knowledge cutoff: April 13, 2024, ensuring models don't have access to answers.

# Model Details

- LLMs Evaluated: GPT-3.5-TURBO, FLAN-T5, LLAMA (various versions), MISTRAL, GEMMA, PHI2.
- Categories: Small (0.08B–3B), Medium (7B–13B), Large (65B–70B).
- Fine-Tuning: Instruction-tuning or reinforcement learning from human feedback.

# Experiments

- Setup: Zero-shot, four-shot, and four-shot with two unsure shots settings.
- Evaluation Metrics: NCR, UR, Ccorrect, Cwrong.
- Comparison: Different model sizes, base vs. fine-tuned models.

# Results

## Factuality

- GPT-3.5-TURBO: Most reliable overall but not consistently correct on seen knowledge.
- FLAN-T5-0.78B: Most reliable for unseen knowledge but unreliable for seen knowledge.

## Consistency

- LLAMA models: High consistency on seen knowledge but lower on unseen knowledge.
- Fine-tuning: Does not significantly improve consistency.

# Performance Comparisons with Prior Work

- Metrics: Net Correct Rate (NCR) reveals gaps in factuality.
- Observations: Larger models more consistent but risk producing consistent wrong information.

# Discussion

- Insights: Larger LLMs perform better on seen knowledge but struggle with unseen knowledge.
- Limitations: Fine-tuning and ICL improve unseen knowledge performance but not consistency.

# Conclusion

- Summary: Defined criteria and metrics for LLMs as KBs, evaluated 26 models.
- Findings: GPT-3.5-TURBO most reliable, model size impacts performance.
- Future Work: Develop strategies to ensure both factuality and consistency in LLMs.

# References

- Petroni et al., 2019
- Sun et al., 2023
- Wang et al., 2021
- Roberts et al., 2020
- He et al., 2024

# Acknowledgements

- Acknowledge contributions and funding sources.

# Q&A

- Invitation for Questions
