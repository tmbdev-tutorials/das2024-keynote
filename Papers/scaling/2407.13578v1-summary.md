# ONE SENTENCE SUMMARY:
Large Language Models (LLMs) show limited reliability as knowledge bases (KBs), struggling with factuality and consistency, especially with unseen knowledge.

# MAIN POINTS:
1. LLMs like GPT-3.5-turbo are not fully factual or consistent.
2. Studies mainly use knowledge graphs and QA datasets for evaluation.
3. Criteria for reliable LLMs include high factuality and consistency.
4. Proposed metrics include Net Correct Rate (NCR) and Uninformative Rate (UR).
5. Consistency metrics distinguish between correct and wrong responses.
6. New QA dataset UnseenQA assesses LLMs on unseen knowledge.
7. Evaluated 26 LLMs, considering model size and fine-tuning effects.
8. Larger models perform better on seen knowledge, worse on unseen knowledge.
9. Fine-tuning improves unseen knowledge handling but not seen knowledge.
10. In-context learning (ICL) with unsure shots enhances unseen knowledge performance.

# TAKEAWAYS:
1. GPT-3.5-turbo is the most reliable LLM but still has limitations.
2. Larger LLMs are more factual but less reliable with unseen knowledge.
3. Fine-tuning and ICL don't significantly improve consistency.
4. Consistency in LLMs is crucial for their role as KBs.
5. Future research should focus on enhancing LLMs' factuality and consistency.
