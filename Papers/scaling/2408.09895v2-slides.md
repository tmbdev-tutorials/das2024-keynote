# Performance Law of Large Language Models

- Chuhan Wu
- Ruiming Tang
- LLM Researchers

# Introduction

- Guided by scaling law, LLMs have achieved impressive performance.
- Scaling law only provides qualitative loss estimation.
- Need for predicting LLM performance with different training settings.
- Introduction of empirical "Performance Law" for MMLU score prediction.

# Background/Related Work

- Scaling laws predict training loss under certain configurations.
- Existing performance predictions based on model sizes and training data lack accuracy.
- Need for accurate LLM performance estimation for model design and optimization.

# Contributions

- Introduction of the "Performance Law" for MMLU score prediction.
- Empirical equation based on key hyperparameters and training data size.
- Accurate performance predictions for various LLMs across different sizes and architectures.

# Objective

- Predict the MMLU score of LLMs using key hyperparameters and training data size.
- Provide a tool to guide LLM architecture choices and resource allocation.

# Methodology Overview

- Performance law for dense models and mixture-of-experts (MoE) models.
- Key variables: number of layers, hidden size, intermediate size of FFN, training data size, model size.
- Introduction of empirical model saturation clip and model unstable discount.

# Performance Law Formulation

- Dense model key variables: number of layers \(N\), hidden size \(h\), intermediate size \(d\), training data \(T\), model size \(S\).
- Model saturation clip: \(T' = \min(T, S)\).
- Model unstable discount: \(u = e^{-\left(\frac{10}{d} + \frac{20}{h}\right)(\gamma N)^2}\).
- MMLU score prediction: 
  \(MMLU = w_1 \log(uN) + w_2 \log(uh) + w_3 \log(ud) + w_4 \log(uT') + b\).

# Model Details

## Dense Model

- Number of layers \(N\)
- Hidden size \(h\)
- Intermediate size \(d\)
- Training data size \(T\)
- Model size \(S\)
- Empirical model saturation clip and unstable discount

## MoE Model

- Number of activated parameters \(A\)
- Expansion factor \(g\)
- MoE unstable discount \(u'\)

# Examples of Performance Prediction

- Python code for predicting performance of dense and MoE models.
- Example predictions for 7B dense model and 141B MoE model.

# Results

- Accurate predictions for various models from 0.5B to 1000+B parameters.
- Strong correlation between predicted and reported MMLU scores.
- Insights derived from performance law implications.

# Performance Comparisons with Prior Work

- Performance law shows better accuracy and generality compared to existing methods.
- Effective for different model structures and shapes.

# Discussion

- Key insights on model depth, hidden size, and FFN size.
- Similar performance of mainstream attention architectures.
- Importance of data quality and distribution.

# Conclusion

- Introduction of performance law for LLMs.
- Accurate MMLU score predictions across different models and years.
- Guidance for LLM architecture design and computational resource allocation.

# References

- Key citations from the paper
  - Kaplan et al., 2020. Scaling laws for neural language models.
  - Hendrycks et al., 2020. Measuring massive multitask language understanding.
  - Touvron et al., 2023. Llama: Open and efficient foundation language models.
  - Srivastava et al., 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.

# Q&A

- Invitation for questions
