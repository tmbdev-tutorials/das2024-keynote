# TEXT RECOGNITION

# Kosmos-2.5 (2023)

- Multimodal model
- Handles text recognition in various document formats
- Designed for OCR-related tasks

[arxiv:2306.14824](https://arxiv.org/abs/2306.14824)

# Nougat (Neural Optical Graph Translator; 2023)

- Converts scientific PDFs to LaTeX
- Handles text recognition and layout interpretation
- No reliance on traditional OCR

[GitHub: Nougat](https://github.com/facebookresearch/nougat)

# TrOCR (Transformer-based OCR; 2021)

- Transformer-based OCR model
- Vision transformer (ViT) encoder with GPT-2-like decoder
- Trained on synthetic and real-world OCR tasks

[arxiv:2109.10282](https://arxiv.org/abs/2109.10282)

# Donut (Document Understanding Transformer; 2022)

- OCR-free model for text recognition
- End-to-end processing directly from document images
- Uses a transformer-based architecture

[arxiv:2111.15664](https://arxiv.org/abs/2111.15664)

# StrucTexT (2022)

- Multi-modal transformer for structured text extraction
- Designed for visually rich documents
- Integrates visual, layout, and textual information

[arxiv:2108.02923](https://arxiv.org/abs/2108.02923)

# TABLE RECOGNITION

# Tabby (Table Understanding Transformer; 2023)

- Transformer-based model for table understanding
- Extracts structured data from tables
- Handles complex document layouts

[arxiv:2305.08475](https://arxiv.org/abs/2305.08475)

# TableFormer (2021)

- Transformer-based model for table structure recognition
- Identifies table cells, headers, and body regions
- Effective for diverse table layouts

[arxiv:2106.11419](https://arxiv.org/abs/2106.11419)

# CascadeTabNet (2021)

- Cascade mask R-CNN-based model
- Table detection and structure recognition
- Progressive detection approach for complex tables

[arxiv:2004.12629](https://arxiv.org/abs/2004.12629)

# TATR (Table Transformer; 2021)

- Transformer-based architecture for table recognition
- Integrates visual and textual information
- Focuses on table boundary detection and structure extraction

[arxiv:2105.12210](https://arxiv.org/abs/2105.12210)

# DeepDeSRT (2020)

- Two-stage deep learning approach for table recognition
- CNN-based model for table detection
- Recognizes internal structure of tables

[arxiv:1908.04729](https://arxiv.org/abs/1908.04729)

# TableNet (2019)

- Fully convolutional network (FCN) for table detection
- Separates table structure detection from text extraction
- Handles diverse table layouts effectively

[arxiv:1903.01949](https://arxiv.org/abs/1903.01949)

# VQA

# VisualMRC (2023)

- Multi-modal model for OCR-free question answering
- Integrates image and layout features
- Handles visual question answering in documents

[arxiv:2302.01065](https://arxiv.org/abs/2302.01065)

# UDOP (Unified Document Pre-training; 2022)

- Unified pre-training approach for OCR-free document understanding
- Supports visual question answering
- Multi-modal inputs (text and image)

[arxiv:2210.05006](https://arxiv.org/abs/2210.05006)

# OCR-free Visual Question Answering (2021)

- OCR-free model for document VQA
- Uses visual and textual information at pixel level
- Handles question answering directly from document images

[arxiv:2106.00310](https://arxiv.org/abs/2106.00310)

# DocVQA (2020)

- Dataset and model for document visual question answering (VQA)
- Uses multi-modal transformers
- Focuses on OCR-free document analysis tasks

[arxiv:2007.00398](https://arxiv.org/abs/2007.00398)

# LAYOUT ANALYSIS

# DiT (Document Image Transformer; 2022)

- Transformer-based model for document image analysis
- Page segmentation with attention mechanisms
- Segments pages into text, images, and tables

[arxiv:2203.02378](https://arxiv.org/abs/2203.02378)

# ERNIE-Layout (2022)

- Multi-modal pre-trained model
- Document-level question answering and layout analysis
- Integrates visual and textual information

[arxiv:2203.06670](https://arxiv.org/abs/2203.06670)

# DocFormer (2021)

- Transformer-based model for multi-modal document processing
- Combines text and image features
- Designed for layout analysis and document understanding

[arxiv:2106.11539](https://arxiv.org/abs/2106.11539)

# PubLayNet (2019)

- Dataset and model for document layout analysis
- Faster R-CNN-based approach
- Segments document pages into text, figures, tables, captions

[arxiv:1908.07836](https://arxiv.org/abs/1908.07836)

# UNet-based Document Layout Analysis (2020)

- UNet architecture for semantic segmentation
- Detects regions such as text, tables, and images
- Handles document layout analysis tasks

[arxiv:2002.09734](https://arxiv.org/abs/2002.09734)

# PREPROCESSING

# OCRDiff (2023)

- Diffusion model for OCR enhancement
- Improves OCR accuracy in noisy conditions
- Generates enhanced text images for better recognition

[arxiv:2303.04292](https://arxiv.org/abs/2303.04292)

# DeepRuler (2021)

- Reading order detection in complex layouts
- Uses deep learning to establish reading sequence
- Useful for preprocessing multi-column documents

[arxiv:2104.07012](https://arxiv.org/abs/2104.07012)

# SCENE TEXT

# CRAFT (Character Region Awareness for Text Detection; 2019)

- Detects individual character regions in natural scenes
- Links characters into text lines
- Effective for scene text detection in complex settings

[arxiv:1904.01941](https://arxiv.org/abs/1904.01941)

# TextDragon (2019)

- Detects curved text in natural scene images
- Uses CNN and deformable convolution networks
- Handles text with complex shapes

[arxiv:1911.02398](https://arxiv.org/abs/1911.02398)

# Mask TextSpotter (2018)

- End-to-end scene text detection and recognition
- Uses a mask R-CNN-based framework
- Designed for spotting text with arbitrary shapes

[arxiv:1807.02242](https://arxiv.org/abs/1807.02242)

# HANDWRITING RECOGNITION

# QAHED (Question Answering in Handwritten Documents; 2022)

- OCR-free question answering in handwritten documents
- Extracts relevant information without explicit OCR
- Handles handwritten text directly

[arxiv:2202.04410](https://arxiv.org/abs/2202.04410)

# HATR (Handwriting-Aware Transformer; 2021)

- Handwriting recognition model with transformers
- Captures local and global context in handwritten text
- Effective for complex handwriting styles

[arxiv:2109.09098](https://arxiv.org/abs/2109.09098)

# Seq2Seq HTR (2020)

- Sequence-to-sequence model for handwriting recognition
- Converts sequences of handwritten strokes to digital text
- Handles diverse handwriting styles

[arxiv:2005.05432](https://arxiv.org/abs/2005.05432)

# HWR (Handwriting Recognition in Historical Documents; 2019)

- Focused on handwriting recognition in historical documents
- Addresses challenges like text degradation and varying styles
- Uses deep learning for improved accuracy

[arxiv:1910.05593](https://arxiv.org/abs/1910.05593)

# OTHER

# GraphDoc (2023)

- Graph-based transformer model
- Captures relational structure in document images
- Enhances layout understanding and structure extraction

[arxiv:2303.05284](https://arxiv.org/abs/2303.05284)

# GTE (Graph-based Table Extraction; 2022)

- Graph-based approach for table extraction
- Treats table elements as nodes and edges in a graph
- Designed for structured document extraction

[arxiv:2202.13237](https://arxiv.org/abs/2202.13237)

# VILA (Vision-based Layout Analysis; 2021)

- Vision-based model for layout analysis
- Segments document pages into text, images, tables
- Uses visual features for accurate segmentation

[arxiv:2104.01538](https://arxiv.org/abs/2104.01538)
